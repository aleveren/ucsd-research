{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "from utils import nice_tree_plot, niceprint, permute_square, invert_permutation, without_diag\n",
    "from compute_pam import compute_combo_tensor_pam, IndividualNodeAlphaCalc\n",
    "from example_graphs import make_tree\n",
    "from sim_data import PAMSampler, topics_griffiths_steyvers\n",
    "from lda_collapsed_gibbs import CollapsedGibbs\n",
    "from tree_extraction import Aho\n",
    "from tree_extraction.Aho import get_ratio_matrix\n",
    "from alpha_extract import AlphaExtract\n",
    "from param_stats import topic_difference, find_flat_permutation, find_structural_permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.insert(0, '../anchor-word-recovery/')\n",
    "# from learn_topics import Analysis as AnchorAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. Set up parameters of the model (tree structure, alphas, and topic definitions -- anything else?)\n",
    "2. Compute \"true\" co-occurrence matrix\n",
    "3. Generate some simulated data by sampling from the model\n",
    "  - Parameters: Number of documents, distribution over document sizes\n",
    "4. Compute \"empirical\" co-occurrence matrix (using known per-word-slot topic assignments).  Compare to \"true\" co-occurrence matrix.\n",
    "5. Apply LDA or anchor words to the simulated data -- estimate topics and co-occurrence matrix.  Compare \"estimated\" co-occurrence matrix to \"true\" and \"empirical\" matrices.  Compare \"estimated\" topics to \"true\" topics.\n",
    "  - Parameters: number of topics; any other parameters needed for the chosen topic-estimation algorithm\n",
    "6. Compute ratio matrix\n",
    "7. Derive triplet constraints from ratio matrix\n",
    "8. Apply Aho's algorithm to constraints, producing a tree where each leaf corresponds to a topic.  Compare to \"true\" tree structure.\n",
    "9. Extract alpha parameters from co-occurrence matrix.  Compare to \"true\" alpha parameters.\n",
    "  - Parameters: $\\alpha_{max}$ and $\\delta_{min}$ corresponding to hypothesis class\n",
    "  \n",
    "TODO: Find a way to remove # of topics as a parameter (eg, measure perplexity, and loop over different of # topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# self.aa = AnchorAnalysis(\n",
    "#     infile = TODO,\n",
    "#     vocabfile = TODO,\n",
    "#     outfile = None,\n",
    "#     loss = \"L2\",\n",
    "#     K = num_topics,\n",
    "#     seed = 100,\n",
    "#     eps = 1e-6,\n",
    "#     new_dim = 1000,\n",
    "#     max_threads = 8,\n",
    "#     anchor_thresh = 100,\n",
    "#     top_words = 10,\n",
    "# )\n",
    "# self.aa.run()\n",
    "# return aa.A, aa.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_short_corpus(docs):\n",
    "    corpus_short = []\n",
    "    for doc in docs:\n",
    "        ctr = Counter(doc)\n",
    "        doc_short = [(k, v) for k, v in ctr.items()]\n",
    "        corpus_short.append(doc_short)\n",
    "    return corpus_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empirical_cooccur(sampler, leaf_to_index):\n",
    "    num_topics = len(sampler.topics)\n",
    "    p = np.zeros((num_topics, num_topics), dtype='float')\n",
    "    for node_selections in sampler.doc_nodes:\n",
    "        theta = np.zeros(num_topics, dtype='float')\n",
    "        counter = Counter(node_selections)\n",
    "        for node, count in counter.items():\n",
    "            theta[leaf_to_index[node]] += count\n",
    "        theta /= theta.sum()\n",
    "        p += np.outer(theta, theta)\n",
    "    p /= p.sum()\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_alpha_func(g, alpha_dict):\n",
    "    child_mapper = {n: [np.array(alpha_dict[c]) for c in g.neighbors(n)]\n",
    "                    for n in g.nodes() if g.out_degree(n) > 0}\n",
    "    def f(node):\n",
    "        return child_mapper[node]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topic_func(true_topics, leaf_to_index):\n",
    "    def topic_func(node):\n",
    "        return true_topics[leaf_to_index[node], :]\n",
    "    return topic_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analysis(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            # Data generation params:\n",
    "            true_tree,\n",
    "            true_alphas,\n",
    "            num_docs,\n",
    "            words_per_doc,\n",
    "            vocab_size,\n",
    "            true_topics,\n",
    "            leaf_to_index,\n",
    "            # Extraction params:\n",
    "            num_topics_to_train,\n",
    "            delta_min,\n",
    "            alpha_max,\n",
    "            custom_prior):\n",
    "\n",
    "        self.true_tree = true_tree\n",
    "        self.true_alphas = true_alphas\n",
    "        self.num_docs = num_docs\n",
    "        self.words_per_doc = words_per_doc\n",
    "        self.vocab_size = vocab_size\n",
    "        self.true_topics = true_topics\n",
    "        self.leaf_to_index = leaf_to_index\n",
    "        self.num_topics_to_train = num_topics_to_train\n",
    "        self.delta_min = delta_min\n",
    "        self.alpha_max = alpha_max\n",
    "        self.custom_prior = custom_prior\n",
    "\n",
    "        self.true_num_topics = true_topics.shape[0]\n",
    "    \n",
    "    def run(self):\n",
    "        self.alpha_calc = IndividualNodeAlphaCalc(tree = self.true_tree, values = self.true_alphas)\n",
    "        self.true_cooccur = compute_combo_tensor_pam(g = self.true_tree, alpha = self.alpha_calc)\n",
    "        \n",
    "        # Generate data\n",
    "        self.sampler = PAMSampler(\n",
    "            g = self.true_tree,\n",
    "            num_docs = self.num_docs,\n",
    "            words_per_doc = self.words_per_doc,\n",
    "            vocab_size = self.vocab_size,\n",
    "            alpha_func = make_alpha_func(self.true_tree, self.true_alphas),\n",
    "            topic_func = make_topic_func(self.true_topics, self.leaf_to_index),\n",
    "        )\n",
    "        self.sampler.sample()\n",
    "\n",
    "        # Compute empirical statistics from topic-labeled data\n",
    "        self.emp_cooccur = compute_empirical_cooccur(sampler = self.sampler, leaf_to_index = self.leaf_to_index)\n",
    "        \n",
    "        # Extract topics and co-occurrence from data\n",
    "        self.corpus = make_short_corpus(self.sampler.docs)\n",
    "        self.collapsed_gibbs = CollapsedGibbs()\n",
    "        self.collapsed_gibbs.fit(\n",
    "            corpus = self.corpus,\n",
    "            num_topics = self.num_topics_to_train,\n",
    "            vocab_size = self.vocab_size,\n",
    "            alpha = self.custom_prior,\n",
    "        )\n",
    "        self.est_topics = self.collapsed_gibbs.topics_by_sample()[-1]  # TODO: revisit this calculation?\n",
    "        self.est_cooccur = self.collapsed_gibbs.cooccurrence_by_sample()[-1]  # TODO: revisit this calculation?\n",
    "        \n",
    "        # Extract tree & other parameters from data\n",
    "        self.est_ratio_matrix = get_ratio_matrix(self.est_cooccur)\n",
    "        self.threshold = self.delta_min / ((1 + self.alpha_max) * (1 + self.alpha_max + self.delta_min))\n",
    "        self.est_tree = Aho.extract(m = self.est_ratio_matrix, apply_ratio = False, threshold = self.threshold)\n",
    "        \n",
    "        self.est_alpha_extract = AlphaExtract(g = self.est_tree, R = self.est_cooccur)\n",
    "        self.est_alphas = self.est_alpha_extract.extract()\n",
    "        \n",
    "        return self.est_topics, self.est_tree, self.est_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "branching_factors = [2, 2, 2]\n",
    "num_true_topics = np.prod(branching_factors)  # number of topics with which to generate data\n",
    "\n",
    "true_tree = make_tree(branching_factors)\n",
    "index_to_leaf = [x for x in true_tree.nodes() if true_tree.out_degree(x) == 0]\n",
    "leaf_to_index = {leaf: idx for idx, leaf in enumerate(index_to_leaf)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 10  # number of documents to generate\n",
    "# M = 20  # number of words per document\n",
    "# V = 25  # number of words in vocabulary\n",
    "# dim = 5  # size of \"square\" to use in defining topics\n",
    "# assert dim ** 2 == V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000  # number of documents to generate\n",
    "M = 200  # number of words per document\n",
    "V = 100  # number of words in vocabulary\n",
    "dim = 10  # size of \"square\" to use in defining topics\n",
    "assert dim ** 2 == V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_topics = topics_griffiths_steyvers(num_topics = num_true_topics, dimension = dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_alphas = {\n",
    "    1: 1.5,\n",
    "    2: 1.7,\n",
    "    3: 1.9,\n",
    "    4: 1.8,\n",
    "    5: 2.0,\n",
    "    6: 1.7,\n",
    "    7: 1.6,\n",
    "    8: 1.7,\n",
    "    9: 1.8,\n",
    "    10: 1.9,\n",
    "    11: 1.4,\n",
    "    12: 1.7,\n",
    "    13: 1.5,\n",
    "    14: 1.6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main user-selected parameters for training & extraction\n",
    "num_topics_to_train = num_true_topics\n",
    "delta_min = 0.1\n",
    "alpha_max = 10.0\n",
    "custom_prior = 1.3 ** np.arange(len(index_to_leaf))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Analysis(\n",
    "    # Data generation params:\n",
    "    true_tree = true_tree,\n",
    "    true_alphas = true_alphas,\n",
    "    num_docs = N,\n",
    "    words_per_doc = M,\n",
    "    vocab_size = V,\n",
    "    true_topics = true_topics,\n",
    "    leaf_to_index = leaf_to_index,\n",
    "    # Extraction params:\n",
    "    num_topics_to_train = num_topics_to_train,\n",
    "    delta_min = delta_min,\n",
    "    alpha_max = alpha_max,\n",
    "    custom_prior = custom_prior,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:29<00:00, 33.89it/s]\n",
      "Initializing: 100%|██████████| 1000/1000 [00:11<00:00, 84.09it/s]\n",
      "Training:   5%|▌         | 109.41/2010 [24:31<6:56:02, 13.13s/it] "
     ]
    }
   ],
   "source": [
    "a.run();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "ax[0].imshow(a.true_topics)\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(a.est_topics)\n",
    "ax[1].axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "nice_tree_plot(a.true_tree, ax[0])\n",
    "nice_tree_plot(a.est_tree, ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.true_alphas)\n",
    "print(a.est_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cooccurrence matrices\n",
    "specs = [\n",
    "    dict(name='true', cooccur=a.true_cooccur),\n",
    "    dict(name='emp', cooccur=a.emp_cooccur),\n",
    "    dict(name='est', cooccur=a.est_cooccur),\n",
    "]\n",
    "\n",
    "vmin=0.0\n",
    "vmax=np.max([np.max(s[\"cooccur\"]) for s in specs])\n",
    "\n",
    "fig, ax = plt.subplots(len(specs), 1, figsize=(5, 5*len(specs)))\n",
    "for i, spec in enumerate(specs):\n",
    "    ax[i].imshow(spec[\"cooccur\"], vmin=vmin, vmax=vmax)\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(spec[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(a, filename):\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d_at_%H-%M-%S')\n",
    "    filename = filename.replace('.pkl', '_' + timestamp + '.pkl')\n",
    "    print(\"Saving to {}\".format(filename))\n",
    "    obj = {\n",
    "        'timestamp': timestamp,\n",
    "        'true_tree_leaf_to_index': a.leaf_to_index,\n",
    "        'corpus': a.corpus,\n",
    "        'true_num_topics': len(a.sampler.topics),\n",
    "        'est_num_topics': a.est_topics.shape[0],\n",
    "        'true_topics': a.true_topics,\n",
    "        'est_topics': a.est_topics,\n",
    "        'true_tree': a.true_tree,\n",
    "        'est_tree': a.est_tree,\n",
    "        'true_cooccur': a.true_cooccur,\n",
    "        'emp_cooccur': a.emp_cooccur,\n",
    "        'est_cooccur': a.est_cooccur,\n",
    "        'true_alphas': a.true_alphas,\n",
    "        'est_alphas': a.est_alphas,\n",
    "    }\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(a, 'results/results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find permutation that maps estimated topics to true topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = find_flat_permutation(true_topics=a.true_topics, est_topics=a.est_topics)\n",
    "perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(a.est_topics[perm, :])\n",
    "ax.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_cooccur_permuted = permute_square(X = a.est_cooccur, perm = perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(est_cooccur_permuted, vmin=vmin, vmax=vmax)\n",
    "ax.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_ratio_permuted = get_ratio_matrix(est_cooccur_permuted)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(without_diag(est_ratio_permuted))\n",
    "ax.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_tree_plot(Aho.extract(est_ratio_permuted, apply_ratio=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
