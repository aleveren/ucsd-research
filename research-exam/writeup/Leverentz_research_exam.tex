\documentclass{article}

%\usepackage[final]{nips_2016_modified}  % Use this for final version
\usepackage{nips_2016_modified}  % Use this for draft

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{mathtools}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{float}\usepackage{caption}
\usepackage{outlines}
\usepackage{tikz}
\usepackage{dsfont}
\usepackage{empheq}
\usepackage{geometry}
\geometry{margin=1.5in}

\usetikzlibrary{bayesnet,positioning}

\newcommand{\nth}{^{\text{th}}}
\newcommand{\len}{\mathrm{len}}
\newcommand{\indicator}{\mathds{1}}

%\allowdisplaybreaks

\title{Hierarchical Topic Models}

\author{
  Andrew Leverentz \\
  \texttt{aleveren@eng.ucsd.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
In this article, we survey techniques for automatically inferring hierarchical structures from unlabeled collections of text documents.
We begin with a brief overview of probabilistic topic modeling, as implemented in models such as Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA).
After discussing some limitations of these models, we discuss a line of research which has extended the LDA model and has attempted to address some of its weaknesses.
This approach treats topics as belonging to a potentially infinite tree; in this context, Bayesian non-parametric techniques based on Dirichlet Processes are used to automatically select the structure of the tree.
We compare and contrast two main models that have been used for this approach, and we discuss several inference algorithms that have been published in the literature.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

In recent decades, information-sharing networks such as the World Wide Web and other digital archives have led to the creation of vast collections of electronic textual data.
As these collections grow, the problem of efficiently discovering relationships between documents, or between  user queries and documents, has become more prominent.
The field of \emph{topic modeling} has produced several automated approaches to solving this problem.

One underlying theme of topic modeling is the notion that words with similar meanings tend to co-occur with similar sets of words.
However, due to ambiguities in natural language and variances between different authors, these co-occurrence patterns are not rigid and deterministic.
Hence, many techniques within the field of topic modeling use a probabilistic framework.
This probabilistic framework typically treats topics as discrete probability distributions over vocabulary words, and documents may contain mixtures of multiple topics.

In subsequent sections, we briefly review the origins of Latent Dirichlet Allocation (LDA) and the probabilistic framework for topic modeling.
Then, we trace the development of a line of research which extends the LDA model and is capable of producing not just a flat collection of topics but a nested hierarchy of topics.
We conclude with a discussion of potential avenues for future research.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation}
Two of the main applications of topic models are (1) discovering relations between documents and (2) indexing large collections of documents.

\paragraph{Discovering relations between documents:}
We often want to determine which sets of documents within a corpus share the same topic or subject matter.
One approach is to formulate this as a clustering task with soft cluster assignments, where each document can be assigned to a mixture of multiple topics or clusters.
For example, a news article about a national politician's economic policies might be represented as a mixture between topics corresponding to both \emph{politics} and \emph{economics}.
Thus, if we are looking for related documents, we could return the set of documents which also contain some significant proportion of either \emph{politics} or \emph{economics}.

\paragraph{Indexing large collections of documents:}
In the context of information retrieval, the task is to take a user query (expressed perhaps as a collection of keywords or as a natural-language sentence) and efficiently find a set of documents which are deemed most relevant to that query.

These tasks are complicated by ambiguities in natural language.
In particular, \emph{synonymy} (in which multiple words have nearly identical meanings) and \emph{polysemy} (in which a single word may have multiple meanings, depending on the context) make it difficult to draw conclusions about relationships between documents based on the specific set of words used in each document.
For example, two closely related documents might express nearly the same concept using completely different words, whereas two unrelated documents might coincidentally both use a particular word in different senses.
Because of this, it is necessary to move beyond superficial representations of documents (such as strings of characters or tokens, or histograms of vocabulary words) and instead represent documents using some notion of ``latent semantics,'' or underlying meaning.

Topic models accomplish this by treating topics as probability distributions over the vocabulary and allowing documents to draw from a mixture of multiple topics.
For example, a topic relating to sports would assign relatively high probability to words such as ``player,'' ``team,'' and ``game.''
Similarly, a topic about medicine would assign relatively high probability to ``doctor,'' ``illness,'' and ``pharmacy.''
Then, a document about a basketball player recovering from an injury might consist of $70\%$ \emph{sports} and $30\%$ \emph{medicine}, whereas a document about physical therapy for athletes might consist of $70\%$ \emph{medicine} and $30\%$ \emph{sports}.

Hierarchical topic models extend this idea by acknowledging that whether or not two documents share the same topic may depend on the level of abstraction that the user is interested in.
By explicitly treating the set of available topics as a tree, hierarchical topic models can therefore represent documents as mixtures of nested topics at varying levels of abstraction.
For example, in some contexts, we might consider articles about the San Diego Padres and articles about the Chicago Bulls to belong to the same topic (namely, \emph{sports}), whereas in other contexts, they belong to distinct topics (\emph{baseball} versus \emph{basketball}).
For indexing tasks, hierarchical topic models allow the possibility of expanding or narrowing the set of relevant documents, depending on the level of abstraction desired by the user.
For example, when searching for documents related to the keywords ``Alan Turing,'' the default system behavior might be to display documents related to \emph{computer science}, with the option of narrowing matches to a more specific subtopic (e.g., \emph{British computer scientists}) or expanding matches to a more general supertopic (e.g., \emph{science and engineering}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Probabilistic Topic Models}

Before we discuss models which can generate hierarchies of topics, we will first discuss two probabilistic models which generate ``flat'' (that is, non-nested) collections of topics.
These models are Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA).

\subsection{A Non-Probabilistic Precursor: Latent Semantic Analysis}

The first of these, PLSA, was inspired by an earlier, non-probabilistic model known as Latent Semantic Analysis (LSA), which was introduced in \cite{deerwester1990lsa}.
In LSA, a corpus of documents is summarized as a matrix; each row of this matrix represents a document, each column represents a term in the vocabulary, and each entry represents number of times a given term appears in a given document.
If the corpus contains a sufficiently diverse set of documents, then most documents will only contain a small subset of the full vocabulary, and the resulting matrix will be sparse.
After constructing this matrix, it is then transformed according to per-term and per-document statistics.
(Several variants of LSA exist which use different transformations in this step, although a common choice is known as ``tf-idf,'' or ``term frequency--inverse document frequency.'')
Then, the singular value decomposition (SVD) of the transformed matrix is computed.
This represents the matrix as a product of three matrices: a matrix of orthogonal row vectors where each row corresponds to a document, a diagonal matrix containing the so-called singular values, and a matrix of orthogonal column vectors where each column corresponds to a term in the corpus.
If we compute the SVD using a low-rank approximation, this corresponds to truncating the row vectors and the column vectors in the SVD; at the same time, we also discard all but the largest singular values.
The truncated row vectors from the SVD are known as ``latent semantic vectors.''
These vectors are viewed as representations of the documents in a low-dimensional ``latent semantic space.''
Similarities between documents can be computed, for example, using a normalized dot product (also known as the cosine similarity) in the latent semantic space.
This model, as with many other topic models, uses the \emph{bag-of-words} simplification, in which the specific sequence of words in a document is ignored, and all that matters is the frequency of terms within each document.
Topic models which eliminate the bag-of-words simplification are beyond the scope of this review, but they constitute an active area of research.

\subsection{Probabilistic Latent Semantic Analysis}

In abstract terms, PLSA takes a similar conceptual approach, but the latent semantic representations of topics are combined in each document using a generative probabilistic model rather than simply a linear algebraic one.
PLSA uses what Hofmann \cite{hofmann1999plsa} calls an ``aspect model.''
In the aspect model, each document $d$ is associated with a discrete probability distribution $p(z \mid d)$ over $z \in \mathcal Z$, where $\mathcal Z$ is a set of latent factors which are referred to as ``topics.''
Each topic $z \in \mathcal Z$ is in turn associated with a distribution $p(w \mid z)$ over words $w$ in the vocabulary $\mathcal W$.
Thus, the joint probability of documents in a corpus and the words they contain is given by
\begin{align*}
p(w, d)
&= \sum_{z \in \mathcal Z} p(w, z, d) \\
&= p(d) \sum_{z \in \mathcal Z} p(w \mid z) \, p(z \mid d).
\end{align*}

Within this framework, it becomes possible to infer the latent class mixture of each document using a likelihood-optimization algorithm, such as EM (expectation maximization).

It is also worth noting that the ``A'' in LSA and PLSA is sometimes replaced with ``I'' for ``Indexing'' when the primary goal is to efficiently retrieve documents related to a given user query.
Thus the terms ``LSI'' and ``PLSI'' are often used in the Information Retrieval research community.

\subsection{Latent Dirichlet Allocation}

Latent Dirichlet Allocation (LDA) extends PLSA by using the Dirichlet distribution as a Bayesian prior on the space of discrete probability distributions.
The generative model used in LDA is given by:
\begin{alignat*}{2}
\theta_k &\sim \text{Dirichlet}(\alpha) &\qquad&\text{for each topic $k$} \\
\phi_d &\sim \text{Dirichlet}(\beta) &\qquad&\text{for each document $d$} \\
z_{d,n} &\sim \text{Categorical}(\phi_d) &\qquad&\text{for the $n\nth$ word in document $d$} \\
t_{d,n} &\sim \text{Categorical}(\theta_{z_{d,n}}) &\qquad&\text{for the $n\nth$ word in document $d$}
\end{alignat*}
In other words, the per-topic probability distributions over the vocabulary ($\theta_k$) are i.i.d.\ draws from a Dirichlet distribution.
Then, each document is associated with a random draw $\phi_d$ from another Dirichlet distribution; this represents the proportion of topics present in document $d$.
Within each document, we select a topic indicator $z_{d,n}$ for each word according to the probabilities in the vector $\phi_d$, and the vocabulary word $t_{d,n} \in \mathcal W$ at the $n\nth$ position is selected according to the probabilities in the vector $\theta_{z_{d,n}}$.
The plate diagram for this model is shown in Figure~\ref{fig:plate-lda}.
The main advantage of this formulation is that the parameters associated with each topic $\theta_k$ (corresponding to the probability distributions $p(w \mid z)$ in PLSA) do not need to be specified in advance;
instead, they can be inferred from the training data.

%%%%
\begin{figure}[htb]
%
\centering
\begin{tikzpicture}
\node[obs] (t) {$t_{d,n}$};
\node[latent, right=1.5cm of t] (theta) {$\theta_k$};
\node[latent, left=of t] (z) {$z_{d,n}$};
\node[latent, left=of z] (phi) {$\phi_d$};
\node[const, right=of theta] (alpha) {$\alpha$};
\node[const, left=of phi] (beta) {$\beta$};

\edge{alpha}{theta};
\edge{phi}{z};
\edge{beta}{phi};
\edge{theta}{t};
\edge{z}{t};

\centeredPlate{word-plate}{(t)(z)}{$n \in \{1, \ldots, N_d\}$};
\centeredPlate{doc-plate}{(word-plate)(phi)}{$d \in \{1, \ldots, D\}$};
\centeredPlate{topic-plate}{(theta)}{$k \in \{1, \ldots, K\}$};
\end{tikzpicture}
%
\caption{Plate diagram for the LDA model}
\label{fig:plate-lda}
\end{figure}
%%%%

Several important limitations of the LDA model are:
\begin{enumerate}
\item The number of topics must be specified in advance.
Setting this parameter using standard model-selection techniques such as $k$-fold cross-validation can be time consuming.
We will see that this can be addressed via Bayesian nonparametric techniques, in which one or more latent variables are drawn from an infinite-dimensional space.
\item A ``flat'' collection of topics has no built-in notion of levels of abstraction, and so the topics discovered by LDA may be difficult to interpret.
This is because such models may mix abstract and concrete terms within a single topic.
Nested topics allow us to represent containment relationships between relatively abstract topics, such as ``sports in general'' or ``basketball'' and relatively concrete topics, such as ``Michael Jordan'' or ``the 1996 Chicago Bulls.''
The intention is to create more easily interpretable topics, each of which can be associated with a sharply distinguished level of abstraction.
\item The bag-of-words approach fails to account for the ordering of words, even though this may significantly impact the meaning of a document; for example, in the bag-of-words approach ``man bites dog'' is treated as exactly equivalent to ``dog bites man.''
Solutions to this limitation are outside the scope of this report, although as mentioned above this is an active area of research.
\end{enumerate}

\subsection{Inference Algorithms for LDA}

A number of inference algorithms for LDA have been proposed, including EM (expectation--maximization), Gibbs sampling, and variational inference.
We will see that some of the same algorithms can be applied to hierarchical topic models, although our choice of algorithms is constrained somewhat by the increased complexity of these models.

Expectation--maximization (EM) is an example of a general class of optimization algorithms known as majorization--maximization (MM).
It can be used to find maximum likelihood estimates or maximum a posteriori estimates for latent variables.
However, in a fully Bayesian setting, we are often looking for more than a mere point estimate; rather, we seek to approximate the posterior distribution of the latent variables given the observed data.

Gibbs sampling is a particular kind of stochastic sampling technique which can be used in the context of Bayesian modeling to estimate the posterior distribution.
It belongs to the class of Monte Carlo Markov Chain (MCMC) techniques.
This is a ``Monte Carlo'' technique because it involves estimating a quantity by repeatedly drawing samples of a set of random variables.
It is a ``Markov Chain'' technique because it involves a sequence of stochastic updates in which the transition probabilities at each step depend only upon the previous state.
To implement Gibbs sampling, it is necessary to compute the conditional probability distribution of each latent variable given all other latent variables, plus the full set of observed variables.
A single iteration of the Markov chain update in Gibbs sampling involves taking each latent variable one at a time (possibly in a random order), temporarily discarding its current value, computing the conditional probability distribution of the current latent variable, and sampling an updated value from that conditional distribution.
The update rules needed for implementing this technique are often relatively easy to derive (compared to alternatives such as variational inference).
However, the main disadvantage is that it can be difficult to determine when the Markov chain has converged to the true posterior distribution.

Variational inference is an optimization technique that can be used to find an approximation to the posterior distribution.
Typically, it involves searching within a restricted family of functions (such as those which decompose conveniently into a product of univariate factors) to find the function which most closely approximates the true posterior.
The ``closeness'' between probability distributions can be measured by the Kullback-Leibler divergence.
One relatively simple form of variational inference is based on coordinate ascent.
This approach updates the parameters associated with each latent variable's variational distribution, one at a time, until it has converged to a local optimum.
Fortunately, although it may be intractable to compute the full objective function in terms of the Kullback-Leibler divergence, there is a related quantity known as the evidence lower bound (or ``ELBO'') which differs by a constant amount from the true objective function.
This quantity is easier to compute because it avoids intractable sums and integrals that often arise when computing marginal probabilities in complex Bayesian models.
Therefore, rather than directly assessing whether the KL divergence has reached a local minimum, we can instead monitor whether the ELBO has converged to a local maximum.
Coordinate ascent variational inference is particularly convenient for models which use conditional distributions from the exponential family, and which make use of conjugate priors wherever possible.
In such cases, we can derive a simple closed-form update rule for each of the latent variables.
We will see that Bayesian nonparametric models, which involve latent variables drawn from infinite dimensional spaces, require special handling.
For example, incorporating a greedy algorithm into the update rule is one way to avoid computing infinite sums.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hierarchical Topic Models Based on Dirichlet Processes}

In this section, we discuss a line of research in which random variables known as Dirichlet Processes are used to learn a tree-structured hierarchy of topics from a corpus.
The first such technique is known as the Nested Chinese Restaurant Process, and a subsequent enhancement to this technique is called the Nested Hierarchical Dirichlet Process.

\subsection{Nested Chinese Restaurant Process}

The Nested Chinese Restaurant Process (NCRP) is based on an extension of a model known as the Chinese Restaurant Process (CRP), which we will discuss first.

The CRP is a stochastic process which allows us to put a prior distribution on a potentially infinite partition of a dataset.
Here, the term ``stochastic process'' refers to an infinite set of random variables which are indexed by some infinite set.
With nonzero probability, the CRP can assign observations to arbitrarily large numbers of partitions, although for a finite dataset the size of the partition is of course bounded by the number of observations.

The CRP is based on a thought experiment involving a restaurant containing an infinite number of tables, each of which have an infinite capacity.
The tables are indexed starting from 1, and an arbitrary number of customers sequentially arrive at the restaurant.
With probability 1, the first customer is guaranteed to sit at table 1.
All subsequent customers select a table according to the following rules:
\begin{itemize}
\item If $n$ is the total number of previously-seated customers at the restaurant, the $(n+1)\nth$ customer will sit at the next empty table with probability $\frac{\alpha}{n + \alpha}$.
\item If the first $k$ tables are occupied, with the $i\nth$ table containing $m_i$ customers, the $(n+1)\nth$ customer will sit at table $i$ with probability $\frac{m_i}{n+\alpha}$.
\end{itemize}
Here, $\alpha$ is a positive-valued parameter that affects whether there will be relatively few tables containing a large number of customers, or relatively many tables which each contain only a few customers.
In the limit $\alpha \to 0$, there will be a single table with all of the customers.
On the other hand, as $\alpha \to \infty$, the number of occupied tables increases.

Although this thought experiment gives us an intuitive sense for how to generate a random partition, this formulation of the model involves a sequence of random trials in which each trial is dependent on the outcomes of all previous trials.
Thus, for the purpose of deriving an implementation, it is preferable to use a reformulation that involves independent random variables.
One convenient method is to use a \emph{stick-breaking construction}.
We define an infinite sequence of beta-distributed random variables:
\begin{align*}
V_k &\sim \text{Beta}(1, \alpha) \qquad \text{for all $k \geq 1$}.
\end{align*}
Then, we define an infinite sequence of probabilities:
\begin{align}
\pi_k &= V_k \prod_{j=1}^{k-1} (1-V_j).
\label{eq:pi_crp}
\end{align}
We can view the sequence $\pi_k$ as defining a probability distribution over all positive integers.
In the limit where the number of observations approaches infinity, the distribution over partitions defined by the CRP and by this stick-breaking construction are equivalent.
Since the random variables $V_k$ are i.i.d., this formulation makes it easier to derive properties of the CRP; in particular, we will see that this formulation simplifies the derivation of update rules for variational inference algorithms.

In the context of topic models, the CRP can be used as a distribution over indices into an infinite i.i.d.\ set of random variables.
In terms of the restaurant analogy, this corresponds to associating each table with an independent draw from some ``base distribution'' (for example, in topic models, this is often a draw from a Dirichlet distribution); thus, selecting a table corresponds to selecting one value from an infinite set of draws from the base distribution.
In this context, another formulation of this model is to view it as a so-called ``Dirichlet process.''
The Dirichlet process is a probability distribution over probability distributions.
It is parameterized by a positive scalar parameter $\alpha$ and a base distribution, $G_0$.
The name ``Dirichlet process'' comes from the fact that, for any finite partition $(A_1, \ldots, A_n)$ of the domain of $G_0$, the vector $(X(A_1), \ldots, X(A_n))$ is disributed according to a Dirichlet distribution with parameters $(\alpha G_0(A_1), \ldots, \alpha G_0(A_n))$.
Here, the notation $X(A_i)$ or $G_0(A_i)$ represents the probability mass which $X$ (or $G_0$, respectively) assigns to the set $A_i$.
The fact that such a distribution exists was proved in Ferguson \cite{ferguson1973bayesian}, although this result was merely an existential proof.
Sethuraman \cite{sethuraman1994constructive} later provided the explicit stick-breaking construction for the Dirichlet process described above.
As for notation, we can write
\begin{align*}
X &\sim \text{DP}(\alpha, G_0)
\end{align*}
as shorthand for
\begin{alignat*}{2}
\theta_k &\sim G_0 &\qquad& \text{for $k \geq 1$} \\
V_k &\sim \text{Beta}(1, \alpha) &\qquad& \text{for $k \geq 1$} \\
\pi_k &= V_k \prod_{j=1}^{k-1} (1 - V_j) &\qquad& \text{for $k \geq 1$} \\
X &= \sum_{k=1}^\infty \pi_k \delta_{\theta_k} &&
\end{alignat*}
Note that each $\theta_k$ is a member of the domain of the distribution $G_0$, each $V_k$ and $\pi_k$ is a scalar between $0$ and $1$ (with $\sum_k \pi_k = 1$), and $X$ is a probability distribution with the same domain as $G_0$.
This formulation will be useful in later sections, when we explore the Hierarchical Dirichlet Process (HDP) and the Nested Hierarchical Dirichlet Process (NHDP).

The preceding formulation can be modified slightly if we express it in terms of a so-called GEM distribution (GEM stands for Griffiths, Engen, and McCloskey).
A GEM distribution is a distribution over distributions over positive integers, defined such that
\begin{align*}
\vec \pi &\sim \text{GEM}_2(\gamma_1, \gamma_2)
\end{align*}
is equivalent to
\begin{alignat*}{2}
V_k &\sim \text{Beta}(\gamma_1, \gamma_2) &\qquad& \text{for $k \geq 1$} \\
\pi_k &= V_k \prod_{j=1}^{k-1} (1 - V_j) &\qquad& \text{for $k \geq 1$}
\end{alignat*}
Note in this context that $\vec \pi$ is an infinite sequence; that is, $\vec \pi = \{ \pi_k \}_{k \geq 1}$.
Furthermore, this sequence sums to $1$ and can therefore be viewed as a discrete distribution over the set of positive integers.
The subscript ``2'' indicates the this is a 2-parameter variant of the GEM distribution.
The single-parameter variant is equivalent to setting the first parameter to $1$.
That is, $\text{GEM}(\alpha) = \text{GEM}_2(1, \alpha)$.
With these definitions, a Dirichlet process $X \sim \text{DP}(\alpha, G_0)$ is equivalent to
\begin{alignat*}{2}
\theta_k &\sim G_0 &\qquad& \text{for $k \geq 1$} \\
\vec \pi &\sim \text{GEM}(\alpha) && \\
X &= \sum_{k=1}^\infty \pi_k \delta_{\theta_k} &&
\end{alignat*}

Next, we define Nested Chinese Restaurant Process (NCRP) in terms of the CRP.
Continuing the restaurant analogy, suppose we have an infinite number of restaurants, each with an infinite number of infinite-capacity tables.
There is one restaurant which is singled out as the ``root'' restaurant, and every customer begins by selecting a restaurant from this table according to the rules of the (non-nested) CRP.
Each table has a card containing the address of another restaurant.
After all customers at the current restaurant have been seated, the customers at each table simultaneously get up and move to the restaurant which is marked on the card at their table.
Then, this process repeats at a new set of restaurants.
No restaurant appears on a card more than once, which means that there is one and only one way to reach each restaurant.

We can imagine each restaurant as a node in an infinitely wide, infinitely deep tree.
Each node can be indexed by a tuple representing the sequence of table selections required to reach that restaurant.
For example, the empty sequence corresponds to the restaurant at the root node, and the sequence $(3, 1)$ corresponds to the first descendant of the third descendant of the root node.

In this formulation, a single draw from the NCRP corresponds to a distribution over infinite paths in this tree.
Equivalently, we can think of a single NCRP draw as a probability distribution over infinite sequences of positive integers.
Drawing an infinite sequence $\{a_i\}$ corresponds to choosing table $a_1$ from the first restaurant, entering the restaurant corresponding to $a_1$, choosing table $a_2$ from that restaurant, entering the restaurant corresponding to $a_2$, choosing table $a_3$ from that restaurant, and so on.

Like the CRP, the NCRP can also be defined in terms of a stick-breaking construction.
To do this, we use an infinite collection of beta-distributed random variables, indexed by the set of finite-length paths in an infinite tree:
\begin{align*}
V_r &\sim \text{Beta}(1, \alpha),
\end{align*}
where $r$ is any finite-length tuple of positive integers.
However, as a special case, the random variable associated with the root node, $V_{()}$, is always equal to $1$, rather than being beta-distributed.
Then, we recursively define a set of variables as follows:
\begin{align*}
\pi_{()} &= V_{()} = 1 \\
\pi_{r[1:\ell]} &= \pi_{r[1:\ell-1]} \cdot \left( V_{r[1:\ell]} \prod_{j=1}^{r[\ell]-1} (1-V_{r[1:\ell-1],j}) \right)
\end{align*}
Expanding this recurrence relation yields:
\begin{align*}
\pi_r
&= \prod_{\ell = 1}^{\len(r)} \left( V_{r[1:\ell]} \prod_{j=1}^{r[\ell]-1} \left( 1 - V_{r[1:\ell-1],j} \right) \right) \\
&= \left( \prod_{\ell = 1}^{\len(r)} V_{r[1:\ell]} \right) \left( \prod_{\ell = 1}^{\len(r)} \prod_{j=1}^{r[\ell]-1} \left( 1 - V_{r[1:\ell-1],j} \right) \right).
\end{align*}
If $r$ is a finite-length tuple, then $\pi_r$ represents the probability of drawing a path which contains the prefix $r$.
Since we have fixed $V_{()} = 1$, this implies that $\pi_{()}$, corresponding to the fact that every path contains the empty tuple as a prefix.
Note the similarity between this formulation and equation \eqref{eq:pi_crp}; in the NCRP, we are essentially repeating the CRP at each node in an infinite tree.

The preceding discussion uses a tree of infinite depth.
However, it is also possible to use only a finite number of layers in the tree (that is, to truncate the tree at a finite depth).
Specifically, if we truncate the tree at depth $L$, then each full-length path in the tree is a tuple of positive integers of length $L$.
Note that even if we truncate the tree in terms of its depth, it still contains an infinite number of paths, since the width of the tree is infinite.

There is some ambiguity in the terminology used in the literature, as the term ``NCRP'' can refer to the probability distribution defined above, but it has also been used to refer to a particular nested topic model defined in Blei et al \cite{blei2010ncrp}.
In the NCRP topic model, a single NCRP is used to define a global probability distribution over paths.
Moreover, for each node in the infinite tree, a distribution over the vocabulary is drawn from a Dirichlet.
Each document is associated with a single path through the tree, as well as a distribution over ``levels of abstraction'' (i.e., a distribution over depths within the tree).
Within a document, each word is drawn using a two step process: first, a depth $k$ is selected from the distribution over depths specific to the current document; then, a vocabulary word is drawn from the $k\nth$ topic along the sampled path corresponding to the current document.

Thus, the generative process for sampling documents according to the NCRP is as follows:
\begin{outline}
\1 For each node in the infinite tree, generate a ``topic,'' which consists of a distribution over the vocabulary, by drawing from a Dirichlet.
To avoid actually computing an infinite number of topics, this step can be performed via lazy evaluation.
That is, we can generate and store a new topic only when a subsequent step requires it.
%
\1 Draw a distribution over paths from the NCRP.
In concrete terms, this means we must draw a beta-distributed random variable for each node in the infinite tree.
Again, this can be done via lazy evaluation.
%
\1 For each document:
  \2 Draw a distribution over depths, using a 2-parameter GEM distribution.
  \2 Draw a path through the global tree, using the global distribution over paths.
  \2 For each word in the document:
    \3 Draw a depth $k$ from the distribution-over-depths associated with this document.
    \3 Draw a vocabulary word from the topic associated with the $k\nth$ element of the path associated with this document.
\end{outline}

This generative process allows us to compute the likelihood of a particular document, assuming that we know the global distribution over paths and the global set of topics.
However, we are typically more interested in making inferences based on the posterior distribution---that is, the distribution of the latent variables given the observed data.
In this case, the particular vocabulary words appearing in the corpus are the only observed random variables; all others (including the beta-distributed random variables and the Dirichlet-distributed topics) are latent variables.
We will see that, relative to the LDA model, the task of computing the posterior distribution in the NCRP model is complicated by the fact that there are infinitely many latent random variables (for example, there is one latent beta-distributed random variable for each node in the infinite tree).

Here are the conditional probability distributions for this model, under the assumption that the depth of the hierarchy has been truncated to $L$:
\begin{align*}
W_r &\sim \mathrm{Dirichlet}(\alpha^{(W)}) \\
V_r &\sim \mathrm{Beta}(1, \alpha^{(V)}) \\
\theta_d &\sim \mathrm{Dirichlet}(\alpha^{(\theta)}) \\
z_{d,n} &\sim \mathrm{Categorical}(\theta_d) \\
c_d &\sim \sum_{\substack{r:\text{path} \\ \text{of length } L}} \pi_r \delta_r \\
t_{d,n} &\sim \mathrm{Categorical}(W[c_d[z_{d,n}]])
\shortintertext{where}
%\pi_r &= \left( \prod_{\ell=1}^{L} V_{r[1:\ell]} \right) \left( \prod_{\ell=1}^{L} \prod_{j=1}^{r[\ell]-1} (1 - V_{r[1:\ell-1],j}) \right)
\pi_r &= \prod_{\ell=1}^{\len(r)} \left( V_{r[1:\ell]} \prod_{j=1}^{r[\ell]-1} (1 - V_{r[1:\ell-1],j}) \right)
\end{align*}
The plate diagram for this graphical model is shown in Figure~\ref{fig:plate-ncrp}.

%%%%
\begin{figure}[htb]
%
\centering
\begin{tikzpicture}
\node[obs] (t) {$t_{d,n}$};
\node[latent, above=of t] (c) {$c_d$};
\node[latent, right=1.5cm of c] (v) {$V_r$};
\node[latent, right=1.5cm of t] (w) {$W_r$};
\node[latent, left=of t] (z) {$z_{d,n}$};
\node[latent, above=of z] (th) {$\theta_d$};
\node[const, left=of th] (alphaTheta) {$\alpha^{(\theta)}$};
\node[const, right=of v] (alphaV) {$\alpha^{(V)}$};
\node[const, right=of w] (alphaW) {$\alpha^{(W)}$};

\edge{th}{z};
\edge{z}{t};
\edge{c}{t};
\edge{v}{c};
\edge{w}{t};
\edge{alphaTheta}{th};
\edge{alphaV}{v};
\edge{alphaW}{w};

\centeredPlate{word-plate}{(t)(z)}{$n \in \{1, \ldots, N_d\}$};
\centeredPlate{doc-plate}{(th)(c)(z)(t)(word-plate)}{$d \in \{1, \ldots, D\}$};
\centeredPlate{tree-plate}{(w)(v)}{\begin{tabular}{c}$r: $ path to \\ any node\end{tabular}};
\end{tikzpicture}
%
\caption{Plate diagram for the NCRP model}
\label{fig:plate-ncrp}
\end{figure}
%%%%

The algorithms that have been proposed for inference in the NCRP fall into two main categories: those based on Gibbs sampling, and those based on variational inference.

\subsubsection{Gibbs Sampling for the NCRP}

Initially, papers such as \cite{griffiths2004hierarchical}, used NCRP inference algorithms based on Gibbs sampling.
(In the next section, we will discuss an alternate inference algorithm that was introduced in \cite{wang2009vi_ncrp}.)
In Gibbs sampling, we maintain a state vector which stores values for each of the latent random variables.
Each iteration involves updating the state vector by sampling each of the latent variables, one at a time, according to the following conditional probabilities:
\begin{align*}
z_k &\sim p(z_k \mid \vec x, \vec z_{-k}) \qquad \text{for all latent variables $z_k$.}
\end{align*}
Here, $\vec z_{-k}$ represents the current state vector with the entry for $z_k$ omitted (that is, it represents the current state of all latent variables \emph{except} for $z_k$), and $\vec x$ represents all observed variables.
The long-term behavior of this stochastic system is guaranteed to approach the true posterior distribution, $p(\vec z \mid \vec x)$.
However, it can be difficult to determine whether the state vector has converged.
Moreover, the state vector only represents a single snapshot of the latent variables, as opposed to a distribution over the latent variables, and so we must collect a large number of state-vector samples.
The resulting collection of state-vector samples can then be used as an empirical approximation to the posterior distribution.
Issues such as burn-in (the number of samples to discard before saving the first sample) and lag (the number of samples to discard between saved samples, in order to avoid correlated samples) can also affect the performance of Gibbs sampling.

For some models, it is possible to omit some of the latent variables from the state vector.
This can be done by marginalizing the conditional probabilities over the omitted variables.
One common approach is to marginalize out any global variables (i.e., variables which are shared across all observed data points), leaving only ``local'' per-observation variables in the state vector.
This approach is known as ``collapsed Gibbs sampling,'' and it is used in \cite{blei2010ncrp}.

%**TODO-actual-algorithm??

\subsubsection{Variational Inference for the NCRP}

A variational inference algorithm for the NCRP model was proposed in \cite{wang2009vi_ncrp}.
In general, variational inference involves defining a restricted set of functions and searching for the member of that set which most closely approximates the function we wish to compute---in this case, the true posterior distribution.
This particular algorithm uses reversed Kullback-Leibler divergence to measure the distance between a candidate distribution and the true posterior.
In the paper by Wang et al, the set of approximating distributions $Q$ is the set of all functions which can be written in the following form:
\begin{align*}
q(\vec z) = \prod_{k=1}^m q_k(z_k)
\end{align*}
where $\vec z = (z_1, \ldots, z_m)$ represents the full set of latent variables in the model, and where each function $q_k$ is a member of a parameterized family of univariate distributions.
If we let $\nu_k$ represent the parameters associated with the $k\nth$ latent variable, then the function $q$ is fully specified by selecting a value for $(\nu_1, \ldots, \nu_m)$.
By assuming that the posterior can be approximated as a product of univariate factors, we use what is known as the ``mean-field approximation.''
In mathematical notation, if $p(\cdot \mid x)$ represents the true posterior distribution, we wish to find $q$ such that
\begin{align*}
\mathcal L &= \text{KL}( q \;\Vert\; p(\cdot \mid x) )
\end{align*}
is minimized.
By expanding the definition of $\mathcal L$, we can write
\begin{align*}
\mathcal L &= \log p(x) - \text{ELBO},
\end{align*}
where ELBO (short for ``evidence lower bound'') is defined as
\begin{align*}
\text{ELBO} = E_q[\log p(\vec z, \vec x)] - E_q[\log q(\vec z)]
\end{align*}
Then, since $\log p(x)$ does not depend on $q$, minimizing $\mathcal L$ is equivalent to maximizing ELBO.
This fact will be useful later when we wish to monitor the convergence of our algorithm, because ELBO is signficantly easier to compute than the actual objective function $\mathcal L$.

The actual update rules are derived using coordinate ascent; that is, we repeatedly update the latent variables, one at a time.
In particular, we derive the updates by computing the functional derivative of the objective function with respect to one of the latent variables and solving for the value of the corresponding variational parameter which makes this functional derivative equal to zero.
To aid in deriving the update rules, the model is typically constructed such that all distributions are members of the exponential family, and conjugate priors are used wherever possible.
This is, by design, true of the NCRP model.
Moreover, the variational distributions $q_k$ are assumed to belong to the same family as the corresponding model distribution $p(z_k \mid \text{parents}(z_k))$.

In general, the coordinate-ascent update rule can be derived by calculating the logarithm of the model's joint probability distribution and then performing the following steps for each latent variable $z_k$:
\begin{itemize}
\item Drop any constant terms in the log joint probability (that is, terms that do not depend on $z_k$).
\item Express the remaining terms as a dot product between the ``sufficient statistics'' of the distributon $p(z_k \mid \text{parents}(z_k))$, which are functions of $z_k$, and a vector of ``natural parameters,'' denoted $\eta_k$, which do not depend on $z_k$.
\item Compute the expectation of $\eta_k$ with respect to the variational distributions for the latent variables.
That is, compute
\begin{align*}
E_q[\eta_k] &= E_{z_1 \sim q_1} \left[ \cdots E_{z_m \sim q_m} \left[ \eta_k \right] \cdots \right]
\end{align*}
\item Compute the inverse of the natural parameter transformation corresponding to the variational distribution $q_k$, and apply it to the vector $E_q[\eta_k]$.
This yields the concrete values with which to update the variational parameters corresponding to $z_k$.
\end{itemize}
We then repeat the above steps while continually monitoring the value of the ELBO until it appears to have converged.

The above procedure is sufficient for traditional finite-dimensional models.
However, in the context of Bayesian nonparametric models such as the NCRP, the latent variables belong to an infinite-dimensional space.
This means that there are an infinite number of variational parameters that would need to be updated in each iteration.
Hence, in order to obtain a feasible algorithm, we must make additional approximations.
The approach taken in \cite{wang2009vi_ncrp} is to start with a finite version of the tree which has been truncated in terms of both its depth and its width.
The latent variables associated with the nodes that lie outside of this finite truncation are assumed to be equal to the corresponding prior distribution; thus, outside of the finite truncation, there is no dependence on the variational parameters.
In addition to this, the authors also incorporate greedy heuristics for pruning paths, adding paths, and merging paths in the finite tree.

%***TODO-update-rules-specific-to-NCRP??

Compared to Gibbs sampling, variational inference has the advantange of being deterministic, and its convergence is also significantly easier to detect.
However, variational inference does have its own disadvantages.
Whereas with Gibbs sampling, it is possible to approach the true posterior distribution to an arbitrary level of precision, variational inference can only yield an approximation.
When deriving variational updates, we face a tradeoff: we can choose a more restricted set of variational distributions, which yields an inferior approximation, but leads to easier-to-derive update rules.
On the other hand, if we want a tighter approximation, the update rules are likely to be significantly more complicated.

\subsection{Nested Hierarchical Dirichlet Process}

The primary limitation of the NCRP has to do with the fact that each document is associated with only a single path through the topic hierarchy.
This creates problems when the training corpus contains documents with a variety of ``hybrid'' topics.
From the example mentioned earlier, there may be some documents which are a mixture of \emph{sports} and \emph{medicine}, and the relative proportions of these topics may vary depending on the specific focus of the document.
To accommodate such documents, the NCRP leaves us with limited options:
\begin{enumerate}
\item Treat \emph{sports medicine} as a subtopic of either \emph{sports} or \emph{medicine}.
\item Treat \emph{sports medicine} as a top-level topic, distinct from both \emph{sports} and \emph{medicine}.
\end{enumerate}
Option 1 may lead to unmanageably deep topic hierarchies (or, if we are using the finite-depth variant of NCRP, this option may not even be feasible).
On the other hand, option 2 may lead to unmanageably wide topic hierarchies, particularly if a large number of documents are hybrids of three or more topics.
In either case, it becomes difficult to represent the topic hierarchy as a relatively small tree.

Rather than face these limited options, we would prefer to augment the model so that it can better handle ``hybrid'' topics.
The solution that was proposed in \cite{paisley2015nhdp} is a topic model called the Nested Hierarchical Dirichlet Process (NHDP).
The NHDP, as with the previous topic model, uses a global distribution over paths which is drawn from an NCRP.
Also, there is once again an infinite set of topic vectors (i.e., distributions over vocabulary words) which are arranged in an infinite tree and are independently drawn from a Dirichlet distribution.
As before, each document is associated with a ``local'' distribution over nodes in the infinite tree of topics.
In the NCRP topic model, each document's local distribution consists of nodes along a single path, but in the NHDP, the local distributions can include multiple branches.

To see how the local, per-document distributions are defined, we first discuss the (non-nested) Hierarchical Dirichlet Process (HDP).
The HDP was introduced in \cite{teh2005hdp} to model grouped data, in which there are an unbounded (potentially infinite) number of groups, each of which contains multiple observations and is associated with its own vector of parameters.
A simple, non-nested HDP model can be formulated as follows:
\begin{align*}
H &\sim \text{DP}(\alpha, G_0) \\
H_d &\sim \text{DP}(\beta, H) \\
\theta^*_{d,n} &\sim H_d \\
x_{d,n} &\sim F(\theta^*_{d,n})
\end{align*}
In this formulation, there is a base distribution $G_0$ (such as a Gaussian or a Dirichlet), and a ``global'' distribution $H$ is drawn from a Dirichlet process applied to $G_0$.
The distribution $H$ is discrete, which means that can be viewed as a distribution over countably many independent draws from the base distribution.
Next, per-group distributions $H_d$ (one for each group $d$ in the data) are drawn from another Dirichlet process, which is applied to the global distribution $H$.
For each data point $n$ in group $d$, a parameter vector $\theta^*_{d,n}$ is drawn from the group-specific distribution $H_d$.
Finally, the observed data points $x_{d,n}$ are drawn according to the distribution $F(\theta^*_{d,n})$, where $F$ is some parameterized distribution, such as a Gaussian or a categorical distribution.
In the context of topic models, a ``group'' is often defined to be a single document, and the individual observations within each group are the words within the corresponding document.
The NHDP model uses $G_0 = \text{Dirichlet}(\gamma)$ and $F(\cdot) = \text{Categorical}(\cdot)$.

To define a per-document distribution over nodes, the NHDP uses an HDP defined at each node of the infinite topic tree.
However, it also interleaves a document-specific tree of ``path propagation'' random variables to determine, for each word, whether to extend the current path by drawing the next subtopic or to stop at the current node and draw a specific vocabulary word.

The generative process is as follows:
\begin{outline}[enumerate]
\1 For each node $r$ in the infinite tree, draw a topic vector $W_r$, representing a distribution over the vocabulary.
Since this involves an infinite number of random draws, this step is performed via lazy evaluation.
\1 For each node $r$ and each $j \geq 1$, draw a beta-distributed stick-breaking proportion $V^*_{r,j}$ via lazy evaluation:
\1 For each document $d$:
  \2 For each node $r$ in the infinite tree, perform the following steps via lazy evaluation:
    \3 For each $j \geq 1$:
      \4 Draw an index $z^d_{r,j} \geq 1$ according to a discrete distribution with probabilities given by $\pi_k = V^*_{r,k} \prod_{i=1}^{k-1} (1-V^*_{r,i})$.
      \4 Draw a beta-distributed stick-breaking proportion $V^d_{r,j}$.
  \2 Draw a beta-distributed path-propagation random variable $U^d_r$.
  \2 For each word-slot $n$ in document $d$:
    \3 Let $r = ()$, the empty path.
    \3 Repeat the following steps until a value for $c^d_n$ is chosen:
      \4 Draw a Bernoulli random variable with probability of success $U^d_r$.
      \4 If the outcome is 1, then set $c^d_n = r$ and break out of this loop; otherwise, continue.
      \4 For all $j \geq 1$ (via lazy evaluation), find all indices $k$ such that $z^d_{r,k} = j$.  Let $S_j$ be the set of such indices, and define $\mu_j = \sum_{k \in S_j} V^d_{r,k} \prod_{i=1}^{k-1} (1-V^d_{r,i})$.
      \4 Draw an index $\tilde z \geq 1$ from a discrete probability distribution with probabilities given by $\mu_j$ as defined above.
      \4 Extend $r$ by appending $\tilde z$ to it; that is, update $r \gets (r, \tilde z)$.
    \3 Draw a vocabulary word $t^d_n$ from a categorical distribution with probabilities given by $W_{c^d_n}$.
\end{outline}

In summary, we first draw an infinite, global tree of topics ($W_r$) and stick-breaking proportions ($V^*_{r,j}$).
Then, for each document $d$, we draw local sequences of indices ($z^d_{r,j}$) based on the global stick-breaking proportions.
We also draw an independent set of document-specific stick-breaking proportions ($V^d_{r,j}$).
Together, $z^d_{r,j}$ and $V^d_{r,j}$ determine how the topics descended from node $r$ are rearranged and re-weighted in a document-specific fashion.
Furthermore, we draw ``path-propagation'' probabilities $U^d_r$ for each document, and for each node in the infinite tree.
By combining $z^d_{r,j}$, $V^d_{r,j}$, and $U^d_r$, we have a per-document distribution over an infinite tree of nodes.
For each word $n$ in a given document $d$, we draw a node $c^d_n$ from this document-specific distribution over nodes, and then we draw a vocabulary word $t^d_n$ from ${\text{Categorical}(W_{c^d_n})}$.

To visualize how the NHDP model is constructed, Figure~\ref{fig:plate-nhdp} contains a plate diagram of the Bayesian network underlying this model.
Figure~\ref{fig:prob-nhdp} shows the corresponding conditional probability distributions.

%%%%
\begin{figure}[htb]
%
\centering
\begin{tikzpicture}
\node[obs] (t) {$t^d_n$};
\node[latent, left=of t] (c) {$c^d_n$};
\node[latent, left=of c] (v) {$V^d_{r,j}$};
\node[latent, below=of v] (u) {$U^d_r$};
\node[latent, above=of v] (z) {$z^d_{r,j}$};
\node[latent, left=of z] (vstar) {$V^*_{r,j}$};
\node[latent, below=of u] (w) {$W_r$};
\node[const, left=of vstar] (alphaVstar) {$\alpha^{(V^*)}$};
\node[const, left=3cm of w] (alphaW) {$\alpha^{(W)}$};
\node[const, left=3cm of v] (alphaV) {$\alpha^{(V)}$};
\node[const, left=3cm of u] (alphaU) {$\alpha^{(U)}$};
%
\edge{c}{t};
\edge{z}{c}
\edge{v}{c};
\edge{u}{c};
\edge{vstar}{z};
\edge{w}{t};
\edge{alphaV}{v};
\edge{alphaU}{u};
\edge{alphaVstar}{vstar};
\edge{alphaW}{w};
%
\plate{word-plate}{(t)(c)}{$n \in \{1, \ldots, N_d\}$};
\plate{j-plate}{(z)(vstar)(v)}{$j \geq 1$};
\centeredPlate{tree-plate}{(u)(v)(z)(w)(vstar)(j-plate)}{\begin{tabular}{c}$r: $ path to \\ any node\end{tabular}};
\plate{doc-plate}{(word-plate)(z)(u)(v)}{$d \in \{1, \ldots, D\}$};
\end{tikzpicture}
%
\caption{Plate diagram for the NHDP model}
\label{fig:plate-nhdp}
\end{figure}

\begin{figure}[htb]
\centering
\begin{empheq}[box=\fbox]{align*}
W_r &\sim \text{Dirichlet}(\alpha^{(W)}) \\
V^*_{r,j} &\sim \text{Beta}(1, \alpha^{(V^*)}) \\
V^d_{r,j} &\sim \text{Beta}(1, \alpha^{(V)}) \\
U^d_r &\sim \text{Beta}(\alpha^{(U)}_1, \alpha^{(U)}_2) \\
z^d_{r,j} &\sim \sum_{k \geq 1} \left( V^*_{r,k} \prod_{i=1}^{k-1} (1-V^*_{r,i}) \right) \delta_k \\
c^d_n &\sim \sum_{r: \text{path}} A(r, V^d, z^d) \, B(r, U^d) \, \delta_r \\
A(r, V^d, z^d) &= \prod_{m=0}^{\len(r)-1} \sum_{k \geq 1} \indicator \left[ z^d_{r[1:m],k} = r[m+1] \right] \left( V^d_{r[1:m],k} \prod_{i=1}^{k-1} \left( 1 - V^d_{r[1:m],i} \right) \right) \\
B(r, U^d) &= U^d_r \prod_{m=0}^{\len(r)-1} \left( 1 - U^d_{r[1:m]} \right) \\
t^d_n &\sim \text{Categorical}(W_{c^d_n})
\end{empheq}
\caption{Conditional probability distributions for the NHDP model}
\label{fig:prob-nhdp}
\end{figure}

The inference algorithm for the NHDP model is again based on variational inference.
However, the authors use a variant called ``stochastic variational inference'' (which was introduced in \cite{hoffman2013stochastic}) to allow the algorithm to scale to larger datasets.
This approach uses repeated sampling of mini-batches to update the variational parameters, and in this respect it is similar to stochastic gradient descent.
The update rules for document-specific variables are the same as in traditional variational inference, but the ``global'' (i.e., corpus-wide) variables are updated using the \emph{natural gradient} of the objective function.
The natural gradient update rule has a particularly simple form for exponential-family models, since there is a preconditioning matrix which effectively cancels out.
By using stochastic variational inference, we end up with an algorithm which is, in practice, more efficient than either Gibbs sampling or coordinate-ascent variational inference.
%**TODO-vi-algorithm-for-nhdp?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Hierarchical Topic Models Based on Pachinko Allocation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Directions for Future Research}

We can identify some directions for future research by considering some of the limitations of the NHDP model.
%Some of these areas are already being actively researched in the topic modeling community.

\paragraph{Scalable algorithms:}
The currently available set of inference algorithms force us to make a significant tradeoff between precision and scalability.
On the one hand, we have Gibbs sampling, which can theoretically compute the true posterior distribution with arbitrary precision, but which requires requires many passes over the training data and therefore fails to scale well to massive datasets.
On the other hand, we have stochastic variational inference, which uses a more efficient mini-batch approach and has deterministic convergence, but which requires us to make major approximating assumptions, such as the mean-field approximation.
To improve the inference algorithm, we could perhaps look for a variant of Gibbs sampling that requires fewer passes over the training data.
Alternatively, we might seek a modified version of stochastic variational inference which makes less stringent assumptions on the form of the variational distributions.

\paragraph{Interpreting models:}
The models discussed above generally represent topics as discrete probability distributions over the entire vocabulary.
However, a typical vocabulary may contain tens of thousands of words, which makes the vector of topic probabilities unwieldy and difficult to interpret.
A common approach is to visualize a topic by displaying its top $K$ most likely vocabulary words.
However, within a large topic hierarchy, a small value of $K$ might not provide enough information to disambiguate closely related topics, whereas a large value may lead to information overload.
By comparison, human-curated topic hierarchies such as library cataloguing systems often use a very small number of words to describe each node in a large hierarchy.
Hence, it might be useful to post-process the results of these models in order to derive a concise yet meaningful description of each topic.

\paragraph{Incorporating human feedback:}
It may be useful to be able to incorporate human feedback in cases where the model's quality is deemed subpar.
This would require not only an investigation of the types of mistakes which these topic models are prone to make, but also an investigation into how best to design a user interface that is capable of gathering useful feedback.
It may also be useful to apply semi-supervised or multi-label classification techniques in scenarios where partial or even contradictory labels are available.

\paragraph{Moving beyond the bag-of-words model:}
All of the models discussed above ignore the order in which words appear in a document.
The sequence of words in a document can influence its meaning, and therefore its classification within a topic hierarchy.
By making use of word-order information, we could potentially improve the quality of the results.

%\paragraph{Extending ``anchor words'' to hierarchical topics}

\paragraph{Frameworks for Bayesian non-parametric inference:}
Deriving inference algorithms for Bayesian non-parametric models, such as the ones discussed above, is a laborious process which is difficult to automate.
For instance, even though the NCRP and NHDP topic models share many features, their inference algorithms must be derived in a model-specific way.
This is made especially difficult by the fact that, in Bayesian non-parametric models, there can be an infinite number of latent variables.
This requires us to augment traditional inference algorithms with model-specific heuristics.
An approach that combines black-box inference techniques (such as those used in probabilistic programming languages) with model-agnostic heuristics for handling infinite-dimensional latent variables might make it feasible to efficiently explore and compare many variants of these models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\nocite{*}
%\bibliographystyle{plainnat}
\bibliographystyle{plain}
\bibliography{../bibliography}

\end{document}
