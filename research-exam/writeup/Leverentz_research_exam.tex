\documentclass{article}

%\usepackage[final]{nips_2016_modified}  % Use this for final version
\usepackage{nips_2016_modified}  % Use this for draft

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{mathtools}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{outlines}
\usepackage{tikz}
\usepackage{dsfont}
\usepackage{empheq}
\usepackage{xfrac}
\usepackage{enumitem}
%\usepackage{geometry}
%\geometry{margin=1.5in}

\usetikzlibrary{bayesnet,positioning}

\newcommand{\nth}{^{\text{th}}}
\newcommand{\len}{\text{len}}
\newcommand{\indicator}{\mathds{1}}
\newcommand{\hackystatei}[1]{\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{#1\strut}}
\newcommand{\hackystateii}[1]{\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent-\algorithmicindent}{#1\strut}}

%\allowdisplaybreaks

\title{Hierarchical Topic Models}

\author{
  Andrew Leverentz \\
  \texttt{aleveren@eng.ucsd.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
In this paper, we survey techniques for automatically inferring hierarchical structures from unlabeled collections of text documents.
We begin with an overview of probabilistic topic modeling, as implemented in models such as Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA).
After examining some limitations of these models, we discuss a line of research which has extended the LDA model and has attempted to address some of its weaknesses.
This approach treats topics as belonging to a potentially infinite tree; in this context, Bayesian non-parametric techniques based on Dirichlet processes are used to automatically select the structure of the tree.
We compare and contrast two topic models that have been used for this approach---the Nested Chinese Restaurant Process (NCRP) and the Nested Hierarchical Dirichlet Process (NHDP)---and we discuss several inference algorithms that have been published in the literature.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

In recent decades, large-scale digital archives and information-sharing networks such as the World Wide Web have led to the creation of vast collections of electronic textual data.
As these collections grow, the problem of efficiently discovering relationships between documents, or between  user queries and documents, has become more prominent.
The field of \emph{topic modeling} has produced several automated approaches to solving this problem.

One underlying theme of topic modeling is the notion that words with similar meanings tend to co-occur with similar sets of words.
However, due to ambiguities in natural language and variations between different authors, these co-occurrence patterns are not rigid and deterministic.
To address this, many techniques within the field of topic modeling use a probabilistic framework.
In this probabilistic framework, topics are represented as discrete probability distributions over vocabulary words, and documents may contain mixtures of multiple topics.

In subsequent sections, we briefly review the origins of the probabilistic framework for topic modeling, with particular focus on Latent Dirichlet Allocation (LDA).
Then, we trace the development of a line of research which extends the LDA model and is capable of producing not just a flat collection of topics but a nested hierarchy of topics.
We conclude with a discussion of potential avenues for future research.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation}
Two of the main applications of topic models are (1) discovering relations between documents and (2) indexing large collections of documents.

\textbf{Discovering relations between documents:}
We often want to determine which sets of documents within a corpus share the same topic or subject matter.
One approach is to formulate this as a clustering task with soft cluster assignments---also known as a \emph{mixed membership model}---where each document can be assigned to a mixture of multiple topics or clusters.
For example, a news article about a national politician's economic policies might be represented as a mixture between topics corresponding to both \emph{politics} and \emph{economics}.
Thus, if we are looking for related documents, we could return the set of documents which also contain some significant proportion of either \emph{politics} or \emph{economics}.

\textbf{Indexing large collections of documents:}
In the context of information retrieval, the task is to take a user query (expressed perhaps as a collection of keywords or as a natural-language sentence) and efficiently find a set of documents which are deemed most relevant to that query.

Both of these tasks are complicated by ambiguities in natural language.
As noted in Deerwester et al \cite{deerwester1990lsa}, \emph{synonymy} (in which multiple words have nearly identical meanings) and \emph{polysemy} (in which a single word may have multiple meanings, depending on the context) make it difficult to draw conclusions about relationships between documents based on the specific set of words that appear in each document.
For example, two closely related documents might express nearly the same concept using completely different words, whereas two unrelated documents might coincidentally both use a particular word in different senses.
Because of this, it is necessary to move beyond superficial representations of documents (such as strings of characters or tokens, or histograms of vocabulary words) and instead represent documents using some notion of \emph{latent semantics}, or underlying meaning.

Topic models accomplish this by treating topics as probability distributions over the vocabulary and allowing documents to draw from mixtures of multiple topics.
For example, a topic relating to sports would assign relatively high probability to words such as ``player,'' ``team,'' and ``game.''
Similarly, a topic about medicine would assign relatively high probability to ``doctor,'' ``illness,'' and ``pharmacy.''
Then, a document about a basketball player recovering from an injury might consist of $70\%$ \emph{sports} and $30\%$ \emph{medicine}, whereas a document about physical therapy for athletes might consist of $70\%$ \emph{medicine} and $30\%$ \emph{sports}.

Hierarchical topic models extend this idea by acknowledging that whether or not two documents share the same topic may depend on the level of abstraction that the user is interested in.
By explicitly treating the set of available topics as a tree, hierarchical topic models can therefore represent documents as mixtures of nested topics at varying levels of abstraction.
For example, in some contexts, we might consider articles about the San Diego Padres and articles about the Chicago Bulls to belong to the same topic (namely, \emph{sports}), whereas in other contexts, they belong to distinct topics (\emph{baseball} versus \emph{basketball}).
Thus, \emph{baseball} and \emph{basketball} could be considered subtopics under the broader topic of \emph{sports}.
For indexing tasks, hierarchical topic models allow the possibility of expanding or narrowing the set of relevant documents, depending on the level of abstraction desired by the user.
For example, when searching for documents related to the keywords ``Alan Turing,'' the default system behavior might be to display documents related to \emph{computer science}, with the option of narrowing matches to a more specific subtopic (e.g., \emph{British computer scientists}) or expanding matches to a more general supertopic (e.g., \emph{science and engineering}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Probabilistic Topic Models}

Before we discuss models which can generate hierarchies of topics, we will first discuss two probabilistic models which generate ``flat'' (that is, non-nested) collections of topics.
These models are Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA).

\subsection{A Non-Probabilistic Precursor: Latent Semantic Analysis}

The first of these, PLSA, was inspired by an earlier, non-probabilistic model known as Latent Semantic Analysis (LSA), which was introduced in \cite{deerwester1990lsa}.
In LSA, a corpus of documents is summarized as a matrix; each column of this matrix represents a document, each row represents a term in the vocabulary, and each entry specifies the number of times a given term appears in a given document.
In mathematical notation, we have a matrix $M$ such that the entry $M_{i,j}$ denotes the number of times that the $i\nth$ vocabulary word appears in the $j\nth$ document.
If the corpus contains a sufficiently diverse set of documents, then most documents will only contain a small subset of the full vocabulary, and the resulting matrix will be sparse.
After constructing this matrix, we may optionally transform it according to per-term and per-document statistics.
That is, if $M$ is an $N \times D$ matrix, we compute a new $N \times D$ matrix $X$ whose entries are given by
\begin{align}
X_{i,j} &= f(M_{i,j}, M_{1:D,j}, M_{i,1:N}).
\end{align}
Several variants of LSA exist which use different transformations in this step, as described in \cite{wiki:lsa}, although the original paper \cite{deerwester1990lsa} uses an untransformed matrix, with $X = M$.
Then, the singular value decomposition (SVD) of the matrix $X$ is computed.
This represents the matrix as a product of three matrices:
\begin{align}
X &= U \Sigma V^\top
\end{align}
Here, $U$ and $V$ are matrices of orthogonal column vectors.
In $U$, each row corresponds to a term in the vocabulary, and in the transposed matrix $V^\top$, each column corresponds to a document.
Moreover, $\Sigma$ is a diagonal matrix containing scaling factors known as \emph{singular values}.
If we compute the SVD using a low-rank approximation, this corresponds to truncating the number of columns in $U$ and the number of rows in $V^\top$.
At the same time, we also discard all but the largest singular values in $\Sigma$.
The truncated rows from $U$ (corresponding to terms) and truncated columns from $V^\top$ (corresponding to documents) are known as \emph{latent semantic vectors}.
We can view these vectors as concise representations of terms and documents in a low-dimensional \emph{latent semantic space}.
Using these representations, we can measure the similarity between pairs of documents by computing a normalized dot product (also known as \emph{cosine similarity}) of the corresponding latent semantic vectors.

However, one major limitation of this model is that the latent semantic vectors corresponding to terms and documents do not have an intuitive interpretation.
Consequently, there are relatively few theoretical principles to guide the design of algorithms for (non-probabilistic) LSA.
For example, selecting the transformation $f$ which maps $M$ to $X$ must typically be done by trying many variations and then determining which one happens to yield the most reasonable-looking results in practice.
We will see that subsequent models have addressed this limitation by using a probabilistic framework.

Lastly, we note that this model, as with many other topic models, uses the \emph{bag-of-words} simplification, in which the specific sequence of words in a document is ignored, and all that matters is the frequency of terms within each document.
Topic models which eliminate the bag-of-words simplification are beyond the scope of this survey, but they constitute an active area of research.

\subsection{Probabilistic Latent Semantic Analysis}

In abstract terms, Probabilistic Latent Sematic Analysis (PLSA) takes a conceptual approach that is similar to LSA, but in this case the latent semantic representations of topics associated with terms and documents are combined using a probabilistic model, rather than simply a linear algebraic one.
PLSA, introduced in Hofmann \cite{hofmann1999plsa}, uses an \emph{aspect model} to represent documents as probabilistic mixtures of topics.
The following discussion uses a reformulation, outlined in Barber \cite{barber2012bayesian}, which is equivalent to the original PLSA model.

In this model, the observed dataset is a collection of $D$ documents, and $N_d$ represents the length of document $d$, measured in words.
We assume that these documents share a fixed vocabulary $\mathcal V$, which is a set of words.
Furthermore, we assume that there are a total of $K$ available topics, each of which is represented as a distribution over the words in the vocabulary.
Mathematically, we represent each topic $k \in \{1, \ldots, K\}$ as a vector $\theta_k$ of length $|\mathcal V|$ whose components sum to 1.
Each document $d$ is associated with a mixture of topics $\phi_d$, which we represent as a vector of length $K$ whose components sum to 1, and which we view as a probability distribution over topics.
As with LSA, we will use the bag-of-words assumption to ignore the order in which words appear.

Returning to a previously mentioned example, suppose topic 1 is \emph{sports} and topic 2 is \emph{medicine}.
Then, for a document about sports medicine, with more focus on \emph{sports} than on \emph{medicine}, we might have $\phi_d = (\sfrac{7}{10}, \sfrac{3}{10}, 0, \ldots, 0)$.
If the first two vocabulary words in $\mathcal V$ are $v_1 =$ ``player'' and $v_2 =$ ``doctor,'' then the topic vectors $\theta_1$ (for \emph{sports}) and $\theta_2$ (for \emph{medicine}) ought to satisfy
\begin{alignat}{3}
\theta_{1,1} &> \theta_{1,2}, &&\qquad \text{meaning}\qquad& \theta_{\textit{sports}, \text{``player''}} &> \theta_{\textit{sports}, \text{``doctor''}}, \text{ and} \\
\theta_{2,1} &< \theta_{2,2}, &&\qquad \text{meaning}\qquad& \theta_{\textit{medicine}, \text{``player''}} &< \theta_{\textit{medicine}, \text{``doctor''}}.
\end{alignat}

Once we have defined topic vectors $\theta_k$ for $k \in \{1, \ldots, K\}$ and per-document topic mixtures $\phi_d$ for $d \in \{1, \ldots, D\}$, we assume that documents are generated according to the following probabilistic procedure:
\begin{outline}
\1 For each document $d \in \{1, \ldots, D\}$:
  \2 For each word-slot $n \in \{1, \ldots, N_d\}$:
    \3 Select a topic $z_{d,n} \in \{1, \ldots, K\}$ according to the current document's topic mixture $\phi_d$.
    \3 Select a word $t_{d,n} \in \{1, \ldots, |\mathcal V|\}$ from the vocabulary according to the topic vector indexed by $z_{d,n}$.
\end{outline}

This model can be summarized by the following probability distributions:
\begin{align}
z_{d,n} &\sim \text{Categorical}(\phi_d) &\qquad&\text{for the $n\nth$ word in document $d$} \\
t_{d,n} &\sim \text{Categorical}(\theta_{z_{d,n}}) &\qquad&\text{for the $n\nth$ word in document $d$}
\end{align}
Figure~\ref{fig:plate-plsa} illustrates this model using plate-diagram notation.
In this notation, unshaded circles represent latent (i.e., unobserved) random variables, and shaded circles represent observed random variables.
Uncircled variables denote hyperparameters, which are variables with no associated probability distribution.
Edges represent conditional dependencies between variables, which correspond to the probability distributions listed above.
Finally, the rectangular plates denote repetitions of the nodes and edges that they contain.

%%%%
\begin{figure}[htb]
%
\centering
\begin{tikzpicture}
\node[obs] (t) {$t_{d,n}$};
\node[const, right=1.5cm of t] (theta) {$\theta_k$};
\node[latent, left=of t] (z) {$z_{d,n}$};
\node[const, left=of z] (phi) {$\phi_d$};

\edge{phi}{z};
\edge{theta}{t};
\edge{z}{t};

\centeredPlate{word-plate}{(t)(z)}{$n \in \{1, \ldots, N_d\}$};
\centeredPlate{doc-plate}{(word-plate)(phi)}{$d \in \{1, \ldots, D\}$};
\centeredPlate{topic-plate}{(theta)}{$k \in \{1, \ldots, K\}$};
\end{tikzpicture}
%
\caption{Plate diagram for the PLSA model.}
\label{fig:plate-plsa}
\end{figure}
%%%%

Thus, the joint probability of the PLSA model is
\begin{align}
p(\vec{t}, \vec{z})
&= \prod_{d=1}^D \prod_{n=1}^{N_d} p(t_{d,n} \mid z_{d,n}) \, p(z_{d,n}) \\
&= \prod_{d=1}^D \prod_{n=1}^{N_d} \theta_{z_{d,n}, t_{d,n}} \, \phi_{d, z_{d,n}}
\end{align}
Within this framework, it becomes possible to infer the latent topic mixture of each document using a maximum-likelihood optimization algorithm.
It is typically infeasible to directly optimize the likelihood of the data, but another option is the expectation--maximization (EM) algorithm, which was introduced in Dempster et al \cite{dempster1977em} and summarized concisely in Barber \cite{barber2012bayesian}.
The EM algorithm generally consists of alternating between estimating distributions over the latent topic assignments $z_{d,n}$ (known as the ``E step'') and maximizing a quantity related to the likelihood of the data (known as the ``M step'').
These steps are repeated until the estimates appear to have converged.

\subsection{Latent Dirichlet Allocation}

Since the PLSA model does not specify probability distributions for the unobserved parameters $\phi_d$ and $\theta_k$, Bayesian inference methods are not applicable.
It is also prone to overfitting, meaning that its results can be overly sensitive to the randomness that is inherent in generating or sampling the training data.
To address these problems, Latent Dirichlet Allocation (LDA) extends PLSA by using the Dirichlet distribution as a Bayesian prior on the space of discrete probability distributions.
A simpler version of this model was introduced in Blei et al \cite{blei2003lda}, although the term ``LDA'' now typically refers to the variant proposed by Griffiths and Steyvers \cite{griffiths2004lda}.

Recall that an $m$-dimensional Dirichlet distribution is a continuous distribution over the set of length-$m$ vectors whose components are non-negative and sum to one.
In other words, it is a meta-distribution over discrete distributions over a set of $m$ points.
The Dirichlet distribution is parameterized by a length-$m$ vector of positive real numbers, $\vec \alpha = (\alpha_1, \ldots, \alpha_m)$, and its density over the $m$-dimensional simplex is proportional to
\begin{align}
\text{Dirichlet}(\vec x \mid \vec \alpha) &\propto \prod_{i=1}^m x_i^{(\alpha_i-1)}
\end{align}
The Dirichlet distribution is a higher-dimensional generalization of the beta distribution, whose density is proportional to
\begin{align}
\text{Beta}(x \mid \alpha_1, \alpha_0) &\propto x^{(\alpha_1-1)} (1-x)^{(\alpha_0-1)}
\end{align}

The generative model used in LDA is given by:
\begin{alignat}{2}
\theta_k &\sim \text{Dirichlet}(\alpha) &\qquad&\text{for each topic $k$} \\
\phi_d &\sim \text{Dirichlet}(\beta) &\qquad&\text{for each document $d$} \\
z_{d,n} &\sim \text{Categorical}(\phi_d) &\qquad&\text{for the $n\nth$ word in document $d$} \\
t_{d,n} &\sim \text{Categorical}(\theta_{z_{d,n}}) &\qquad&\text{for the $n\nth$ word in document $d$}
\end{alignat}
In other words, the per-topic probability distributions ($\theta_k$) over the vocabulary are i.i.d.\ draws from a Dirichlet distribution.
Then, each document is associated with a random draw $\phi_d$ from another Dirichlet distribution; this represents the proportion of topics present in document $d$.
Within each document, for each position $n \in \{1, \ldots, N_d\}$, we select a topic indicator $z_{d,n}$ according to the probabilities in the vector $\phi_d$, and we select a vocabulary word $t_{d,n} \in \mathcal V$ according to the probabilities in the vector $\theta_{z_{d,n}}$.
The plate diagram for this model is shown in Figure~\ref{fig:plate-lda}.

%%%%
\begin{figure}[htb]
%
\centering
\begin{tikzpicture}
\node[obs] (t) {$t_{d,n}$};
\node[latent, right=1.5cm of t] (theta) {$\theta_k$};
\node[latent, left=of t] (z) {$z_{d,n}$};
\node[latent, left=of z] (phi) {$\phi_d$};
\node[const, right=of theta] (alpha) {$\alpha$};
\node[const, left=of phi] (beta) {$\beta$};

\edge{alpha}{theta};
\edge{phi}{z};
\edge{beta}{phi};
\edge{theta}{t};
\edge{z}{t};

\centeredPlate{word-plate}{(t)(z)}{$n \in \{1, \ldots, N_d\}$};
\centeredPlate{doc-plate}{(word-plate)(phi)}{$d \in \{1, \ldots, D\}$};
\centeredPlate{topic-plate}{(theta)}{$k \in \{1, \ldots, K\}$};
\end{tikzpicture}
%
\caption{Plate diagram for the LDA model}
\label{fig:plate-lda}
\end{figure}
%%%%

The LDA model has two main advantages over the PLSA.
First, by placing a prior on the topic mixtures, it specifies how new documents can be generated.
Hence, it is possible to apply Bayesian techniques to go beyond a maximum-likelihood point estimate; instead, we can estimate the entire posterior distribution of the parameters $\theta$ and $\phi$ given the observed data.
(We will discuss Bayesian inference algorithms further in the next section.)
Second, the Dirichlet priors effectively act as regularizers, which allow us to quantitatively control the bias-variance tradeoff of the model.
In particular, a stronger (more peaked) prior reduces the flexibility of the model, which in turns reduces overfitting at the cost of producing a more biased model.

Several important limitations of the LDA model are:
\begin{enumerate}
\item Unlike PLSA, the LDA model is not amenable to using the EM algorithm, since the EM update rules for LDA involve sums with prohibitively many terms.
Instead, we must use alternative inference algorithms, which we will explore in the next section.
\item The number of topics must be specified in advance.
Setting this parameter using standard model-selection techniques such as $k$-fold cross-validation can be time consuming.
We will see that this can be addressed via Bayesian non-parametric techniques, in which one or more latent variables are drawn from an infinite-dimensional space.
\item A ``flat'' collection of topics has no built-in notion of levels of abstraction, and so the topics discovered by LDA may be difficult to interpret.
This is because such models may mix abstract and concrete terms within a single topic.
Nested topics allow us to represent containment relationships between relatively abstract topics, such as ``sports in general'' or ``basketball'' and relatively concrete topics, such as ``Michael Jordan'' or ``the 1996 Chicago Bulls.''
The intention is to create more easily interpretable topics, each of which can be associated with a sharply distinguished level of abstraction.
\item A related issue involves extremely common words, such as ``the,'' ``or,'' and ``this.''
These words can lead to noisy results because they are roughly equally likely to appear within any topic.
Consequently, they are often explicitly omitted during pre-processing of the dataset using a manually-curated list of so-called \emph{stopwords}.
In the context of nested topic hierarchies, we will see that this issue can be handled automatically, since nodes at the top of the hierarchy will naturally collect common stopwords; then, there is no need to explicitly filter out stopwords during pre-processing.
\item The bag-of-words approach fails to account for the ordering of words, even though this may significantly impact the meaning of a document; for example, in the bag-of-words approach, ``man bites dog'' is considered exactly equivalent to ``dog bites man.''
Solutions to this limitation are outside the scope of this report, although as mentioned above this is an active area of research.
\end{enumerate}

\section{Bayesian Inference Algorithms for Latent-Variable Models}

Before we generalize the LDA model to accomodate hierarchies of topics, we will first discuss several inference algorithms which are relevant to both the LDA model and to hierarchical topic models.

In Bayesian inference for latent-variable models (such as LDA), our goal is generally to compute a posterior distribution over the latent variables, conditioned on the observed data and any fixed model parameters.
That is, we wish to compute
\begin{align}
\text{Posterior} &= p(\text{latent variables} \mid \text{data}, \text{fixed parameters}).
\end{align}
This, in turn, allows us to compute relevant statistics on the posterior distribution, such as the expected value and variance of the latent variables.

A probabilistic model typically defines both the \emph{likelihood} of the data, given by
\begin{align}
\text{Likelihood} &= p(\text{data} \mid \text{latent variables}, \text{fixed parameters}),
\intertext{and the \emph{prior distribution} of the latent variables, given by}
\text{Prior} &= p(\text{latent variables} \mid \text{fixed parameters}).
\end{align}
Therefore, in abstract terms, computing the posterior distribution is a matter of applying Bayes' rule:
\begin{align}
\MoveEqLeft
\text{Posterior} \\
={}& p(\text{latent variables} \mid \text{data}, \text{fixed parameters}) \\
={}&
\frac{p(\text{data} \mid \text{latent variables}, \text{fixed parameters})
        \times
        p(\text{latent variables} \mid \text{fixed parameters})}
     {p(\text{data} \mid \text{fixed parameters})} \\
={}&
\frac{\text{Likelihood} \times \text{Prior}}
     {p(\text{data} \mid \text{fixed parameters})}
     \label{eq:posterior-fraction}
\end{align}
Since the denominator in equation~\eqref{eq:posterior-fraction} does not depend on the value of the latent variables, we can think of it as merely a normalization constant, and we can compute it by marginalizing the numerator over all combinations of values for the latent variables.
However, this marginalization is often difficult to compute---typically because it involves either an intractable integral or a sum with an unmanageable number of terms.
Because of this, we must use approximate inference algorithms to estimate the posterior distribution.

The main Bayesian inference algorithms that have been proposed for the LDA model include Gibbs sampling and variational inference.
We will see that similar algorithms can be applied to hierarchical topic models, although our choice of algorithms is constrained somewhat by the increased complexity of such models.

\subsection{Gibbs Sampling}

Gibbs sampling belongs to a particular class of stochastic sampling techniques known as Monte Carlo Markov Chain (MCMC) algorithms.
It is a ``Monte Carlo'' technique because it involves estimating a quantity by repeatedly drawing samples from a probability distribution;
furthermore, it is a ``Markov Chain'' technique because it involves a sequence of stochastic updates to some state vector in which the transition probabilities at each step depend only upon the previous state.

To implement Gibbs sampling, it is necessary to compute the conditional probability distribution of each latent variable given all other latent variables, plus the full set of observed variables.
Let $\vec x$ denote the observed data, let $\vec z = (z_1, \ldots, z_m)$ denote the latent variables in the model, and let $\vec z_{-k}$ denote the vector of all latent variables \emph{except} $z_k$.
Then, we must be able to compute the following quantity for each $k \in \{1, \ldots, m\}$:
\begin{align}
p(z_k \mid \vec{z}_{-k}, \vec{x})
\label{eq:complete-conditional}
\end{align}
These quantities are sometimes referred to as the \emph{complete conditionals} of the model.

Gibbs sampling maintains a state vector in which each component corresponds to a latent variable.
We initialize the state randomly and repeatedly update it according to stochastic rules.
A single Markov-chain update involves iterating over the latent variables (optionally, this can be done in a random order).
For each latent variable, we temporarily discard its current value, compute the conditional probability distribution of the current latent variable as given in equation~\eqref{eq:complete-conditional}, and sample an updated value from that conditional distribution.
Then, there are theoretical results which guarantee that the long-term equilibrium distribution of the state vector will correspond to the true posterior (for example, see \cite{resnik2010gibbs}).
After reaching equilibrium, we can estimate posterior-distribution statistics by collecting a sample of state vectors, either by running multiple independent Markov chains in parallel, or by taking a sequence of regularly-spaced samples from a single Markov chain.
Together, these sampled state vectors will constitute an empirical approximation of the posterior distribution.
This procedure is summarized in Algorithm~\ref{alg:gibbs}.

\begin{algorithm}[hbt]
\caption{Gibbs Sampling}
\label{alg:gibbs}
\renewcommand{\labelitemi}{{$\bullet$}}
\renewcommand{\labelitemii}{{\tiny$\blacksquare$}}
\begin{itemize}[leftmargin=*]
\item \textbf{Input:}
  \begin{itemize}
  \item $\vec x$: an observed dataset
  \item $C \geq 1$: number of independent Markov chains to simulate
  \item $S \geq 1$: number of samples to save per Markov chain
  \item $B \geq 1$: number of steps to discard before saving any samples (``burn-in period'')
  \item $L \geq 1$: number of steps to simulate between saved samples (``lag'')
  \end{itemize}
\item \textbf{Output:}
  \begin{itemize}
  \item $\hat Z$: a sequence of $C \times S$ samples which collectively approximate the posterior $p(\vec z \mid \vec x)$
  \end{itemize}
\item \textbf{Procedure:}
\end{itemize}
\begin{algorithmic}
\State Initialize $\hat Z$ as an empty list
\For{$i \in \{1, \ldots, C\}$}
  \State Initialize $\vec z$ to a random vector compatible with the domains of the latent variables
  \For{$j \in \{1, \ldots, B + LS\}$}
    \State Compute the conditional distribution $p(z_k \mid \vec z_{-k}, \vec x)$
    \State Update $z_k$ by sampling from this conditional distribution
    \If{$j \geq B$ and $(j - B) \mathbin{\text{mod}} L = 0$}
      \State Append $\vec z$ to $\hat Z$
    \EndIf
  \EndFor
\EndFor
\State \Return $\hat Z$
\end{algorithmic}
\end{algorithm}

% % TODO: decide whether to include this
% A closely related variant known as \emph{collapsed Gibbs sampling} divides the latent variables into ``local'' and ``global'' variables.
% In the context of topic models, local latent variables can include either per-document or per-word variables.
% In some models, we can integrate out all of the global latent variables, leaving us with a complete conditional that depends only on the local latent variables and the observed data.
% Then, we use Gibbs sampling on the remaining latent variables.
% The result will be an estimate of the posterior distribution over local latent variables, conditioned on the observed data.
% Since this process does not directly estimate the posterior distribution over the global latent variables, additional post-processing steps are necessary to infer values for the global latent variables.

The main disadvantage of Gibbs sampling is that it can be difficult to determine when the Markov chain has reached equilibrium.
Furthermore, compared to stochastic variational inference, which we discuss below, Gibbs sampling can require significantly more iterations through the full dataset.
Its main advantage is that, given enough time, the equilibrium distribution of the Markov chain will exactly match the posterior distribution.

\subsection{Coordinate-Ascent Variational Inference}
\label{sec:cavi}

As outlined in \cite{blei2017variational}, variational inference is an optimization technique that can be used to find an approximation to the posterior distribution.
This technique involves defining a restricted set of functions and searching for the member of that set which most closely approximates the true posterior distribution.
We define a set $Q$ of approximating distributions to be the set of all functions which can be written in the following form:
\begin{align}
q(\vec z) = \prod_{k=1}^m q_k(z_k)
\end{align}
where $\vec z = (z_1, \ldots, z_m)$ represents the full set of latent variables in the model, and where each function $q_k$ is a member of a parameterized family of univariate distributions.
If we let $\nu_k$ represent the parameters associated with the $k\nth$ latent variable, then the function $q$ is fully specified by selecting a value for $(\nu_1, \ldots, \nu_m)$.
By assuming that the posterior can be approximated as a product of univariate factors, we use what is known as the ``mean-field approximation.''

For probabilistic inference, we typically use \emph{reversed Kullback-Leibler divergence} to measure the distance between a candidate distribution and the true posterior.
In mathematical notation, if $p(\cdot \mid x)$ represents the true posterior distribution, we wish to find $q$ such that
\begin{align}
\mathcal L &= \text{KL}( q \;\Vert\; p(\cdot \mid x) )
\end{align}
is minimized.
By expanding the definition of $\mathcal L$, we can write
\begin{align}
\mathcal L &= \log p(x) - \text{ELBO},
\end{align}
where ELBO (short for ``evidence lower bound'') is defined as
\begin{align}
\text{ELBO} = E_q[\log p(\vec z, \vec x)] - E_q[\log q(\vec z)]
\label{eq:elbo}
\end{align}
Then, since $\log p(x)$ does not depend on $q$, minimizing $\mathcal L$ is equivalent to maximizing ELBO.
This fact will be useful later when we wish to monitor the convergence of our algorithm, because ELBO is signficantly easier to compute than the actual objective function $\mathcal L$.

To aid in optimizing the objective function, the model is typically constructed such that its conditional distributions are members of the \emph{exponential family}, and conjugate priors are used wherever possible.
By design, this is true of the LDA model and the hierarchical topic models discussed below.
The exponential family \cite{wiki:expfamily} is a collection of parameterized sets of probability distributions which can be written in the following form, where $x$ represents the domain variable, and $\theta$ represents the distribution's parameters:
\begin{align}
p(x \mid \theta)
&=
h(x) \exp\left( \eta(\theta) \cdot T(x) - A(\theta) \right).
\end{align}
Here, $\eta(\theta)$ is a vector which contains the \emph{natural parameters} of the model, and $T(x)$ is a vector containing the \emph{sufficient statistics} of the model.
To write a distribution in exponential-family form, we can attempt to compute the logarithm of the probability density; then, we try to derive the functional form of $\eta$ and $T$ by examining any terms which depend on both $x$ and $\theta$.
Any terms which only depend on $x$ or only on $\theta$ can be folded into the functions $h(x)$ and $A(\theta)$.
For example, the categorical distribution can be expressed as follows:
\begin{align}
\text{Categorical}(x \mid \theta)
&= \prod_{k=1}^K \theta_k^{\indicator[x=k]} \\
&= \exp\left( \sum_{k=1}^K \indicator[x=k] \log \theta_k \right).
\end{align}
Thus, categorical distributions belong to the exponential family, with
\begin{alignat}{2}
\eta(\theta) &= (\log \theta_k)_{k \in 1:K} &\qquad& \text{(``natural parameters''),} \\
T(x) &= (\indicator[x = k])_{k \in 1:K} &\qquad& \text{(``sufficient statistics''),} \\
h(x) &= 1, \\
A(\theta) &= 0.
\end{alignat}
At this point, it is also worth noting that we will also make use of the \emph{inverse natural parameter transform}, which is the inverse of $\eta(\theta)$.
For example, with the categorical distribution we can extract the discrete probabilities $\theta$ from $\eta$ by applying the softmax function:
\begin{align}
\theta_k = \frac{\exp(\eta_k)}{\sum_{\ell=1}^K \eta_\ell}.
\end{align}

For variational inference, in addition to selecting an exponential-family model, the variational distributions $q_k$ are assumed to belong to the same family as the corresponding model distributions $p(z_k \mid \text{parents}(z_k))$.

The update rules are derived using coordinate ascent; that is, we repeatedly iterate over the latent variables and update them one at a time.
In particular, we derive the updates by computing the functional derivative of the objective function with respect to one of the latent variables and solving for the value of the corresponding variational parameter which makes this functional derivative equal to zero.
To obtain concrete update formulas, we can calculate the logarithm of the model's joint probability distribution and then perform the following steps for each latent variable $z_k$:
\begin{itemize}
\item Drop any constant terms in the log joint probability (that is, terms that do not depend on $z_k$).
\item Express the remaining terms as a dot product between the sufficient statistics of the distribution $p(z_k \mid \text{parents}(z_k))$, which are functions of $z_k$, and a vector of natural parameters, denoted $\eta_k$, which do not depend on $z_k$.
\item Compute the expectation of $\eta_k$ with respect to the variational distributions for the latent variables.
That is, compute
\begin{align}
E_q[\eta_k] &= E_{z_1 \sim q_1} \left[ \cdots E_{z_m \sim q_m} \left[ \eta_k \right] \cdots \right]
\end{align}
\item Compute the inverse of the natural parameter transformation corresponding to the variational distribution $q_k$, and apply it to the vector $E_q[\eta_k]$.
This yields the concrete values with which to update the variational parameters corresponding to $z_k$.
\end{itemize}
We then repeat the above steps while continually monitoring the value of the ELBO until it appears to have converged.
This procedure is summarized in Algorithm~\ref{alg:cavi}.

\begin{algorithm}[hbt]
\caption{Coordinate-Ascent Variational Inference}
\label{alg:cavi}
\renewcommand{\labelitemi}{{$\bullet$}}
\renewcommand{\labelitemii}{{\tiny$\blacksquare$}}
\begin{itemize}[leftmargin=*]
\item \textbf{Input:}
  \begin{itemize}
  \item $\vec x$: an observed dataset
  \end{itemize}
\item \textbf{Output:}
  \begin{itemize}
  \item $(\nu_1, \ldots, \nu_m)$: vector of optimized variational parameters corresponding to the latent variables
  \end{itemize}
\item \textbf{Procedure:}
\end{itemize}
\begin{algorithmic}
\State Initialize $(\nu_1, \ldots, \nu_m)$ to a ``reasonable'' guess based on the input data.
(This can be done using model-specific heuristics, combined with random noise for symmetry-breaking.)
\State Throughout this algorithm, $q$ will be determined by the current value of $(\nu_1, \ldots, \nu_m)$:
\begin{align*}
q(\vec z) = \prod_{k=1}^m q_k(z_k; \nu_k)
\end{align*}
\While{ELBO has not converged}
  \For{each latent variable $z_k$, with $k \in \{1, \ldots, m\}$}
    \State Update $\nu_k \gets \theta_k(E_{z \sim q}[\eta_k])$.
    \hackystateii{Here, $\theta_k(\cdot)$ is the inverse parameter transform corresponding to $z_k$.
    Moreover, $\eta_k$ is the vector of natural parameters associated with the complete conditional $p(z_k \mid \vec z_{-k}, \vec x)$.}
  \EndFor
  \hackystatei{Compute ELBO according equation~\eqref{eq:elbo}.}
\EndWhile
\State \Return $(\nu_1, \ldots, \nu_m)$
\end{algorithmic}
\end{algorithm}

\subsection{Stochastic Variational Inference}
\label{sec:svi}

Stochastic variational inference (which was introduced in \cite{hoffman2013stochastic}) allows variational inference to scale to larger datasets.
It does this by dividing the latent variables into ``local'' (i.e., per-document) and ``global'' (i.e., corpus-wide) latent variables.
In this section, we will use the generic model illustrated in Figure~\ref{fig:svi-local-global} as a prototype, with $\beta$ denoting the global latent variables and $z_{1:N}$ denoting the local latent variables.
Given a model of this structure, we can see the limitations of coordinate-ascent variational inference by considering how to compute the complete conditionals:
\begin{alignat}{2}
&\text{For local variables:}&\qquad& p(z_n \mid \alpha, \beta, z_{-n}, x_{1:N}) = p(z_n \mid \alpha, \beta, x_n) \\
&\text{For global variables:}&\qquad& p(\beta \mid \alpha, z_{1:N}, x_{1:N})
\end{alignat}
In particular, note that if $\alpha$, $\beta$, and $x_n$ are given, then $z_n$ is conditionally independent of all other variables.
Thus, the coordinate-ascent computations for local latent variables can be done efficiently, using one data point at a time.
In contrast, a single update to the global latent variable $\beta$ requires computations involving the entire training dataset.

%%%%
\begin{figure}[htb]
%
\centering
\begin{tikzpicture}
\node[obs] (x) {$x_n$};
\node[latent, left=of x] (z) {$z_n$};
\node[latent, above=0.5cm of z] (beta) {$\beta$};
\node[const, left=of beta] (alpha) {$\alpha$};

\edge{z}{x};
\edge{beta}{z};
\edge{beta}{x};
\edge{alpha}{beta};
\edge{alpha}{z};

\plate{obs-plat}{(x)(z)}{$n \in \{1, \ldots, N\}$};
\end{tikzpicture}
%
\caption{An illustration of the distinction between local and global latent variables in stochastic variational inference.  %
Here, $\beta$ denotes the global latent variables, and $z_{1:N}$ denotes the local latent variables.}
\label{fig:svi-local-global}
\end{figure}
%%%%

To address this inefficiency, stochastic variational inference uses repeated sampling of mini-batches to update the global variational parameters.
The update rules for the local latent variables are the same as in coordinate-ascent variational inference, but the global latent variables are updated using the \emph{natural gradient} of the objective function, estimated on small samples of the data.

Traditional optimization routines use a standard gradient; this make sense in cases where Euclidean distance suffices for measuring similarities between candidate maximizers.
However, in the case of parameterized probability distributions, the Euclidean distance between pairs of parameter vectors may not accurately reflect the ``true'' difference between the corresponding distributions.
Hoffman \cite{hoffman2013stochastic} gives an example involving a normal distribution parameterized by its mean and variance.
If we start with a mean of $\mu = 0$ and a variance of $\sigma^2 = 0.01$, then adding $1$ to the mean will have a relatively large effect, since $\text{Normal}(0, 0.01)$ has very little overlap with $\text{Normal}(1, 0.01)$.
The situation is reversed if we start with $\mu = 0$ and $\sigma^2 = 1000$, since $\text{Normal}(0, 1000)$ is nearly identical to $\text{Normal}(1, 1000)$.
Thus, relying on gradients defined in terms of Euclidean distance can lead to unnecessarily slow convergence, since some steps will essentially be wasted by moving in an inefficient direction.

To address this, we define a matrix $G$ which encodes information about how the ``true'' distance function varies within a local neighborhood.
This matrix $G$ is also known as a \emph{metric tensor}.
When dealing with parameterized probability distributions, we can define $G$ in terms of a symmetric variant of KL divergence---in particular, we can use the arithmetic mean of KL divergence and reversed KL divergence.
We omit the details of the calculation, but $G$ turns out to be equal to a matrix known as the \emph{Fisher information matrix} of the variational distribution.
Then, rather than simply following the gradient of the objective function in Euclidean coordinates (i.e., $\nabla \mathcal L$), the natural gradient takes steps in the direction of $G^{-1} \nabla \mathcal L$.
For exponential-family models, the natural gradient update rule has a particularly simple form, since the inverse metric tensor $G^{-1}$ cancels out entirely in such cases.
In particular, the natural gradient for the global parameters is
\begin{align}
G^{-1} \nabla \mathcal L
&=
E_q[\eta^{\text{global}}] - \mu.
\end{align}
Here, $\mu$ represents the current value of the variational parameters corresponding to the global variable $\beta$.
Moreover, $\eta^{\text{global}}$ represents the exponential-family natural parameters corresponding to the complete conditional of the global variable, $p(\beta \mid \alpha, z_{1:N}, x_{1:N})$.

Now, instead of computing the natural gradient of the objective function based on the entire dataset, we can use stochastic optimization techniques to compute updates based on random subsets of the data called \emph{mini-batches}.
For each observation in each mini-batch, the local per-observation parameters are updated according to the traditional coordinate-ascent formulas.
For the global parameters, only a small modification is needed: we compute natural gradient using a virtual dataset containing multiple copies of the current mini-batch.
In particular, if the full dataset has $N$ observations, and a mini-batch $b$ contains $S$ observations, we perform updates for the global parameters using a virtual dataset consisting of $\lceil N / S \rceil$ copies of the mini-batch.
As long as we base our updates on estimates which agree with the natural gradient \emph{in expectation}, we will still achieve reasonable convergence;
this same principle is also used in stochastic gradient descent.
The update rule for the global parameters is then
\begin{align}
\mu
&\gets \mu + \rho_t G^{-1} \nabla \mathcal L \\
&= (1-\rho_t) \mu + \rho_t E_q[\eta^{\text{global}}_b]
\end{align}
Here $\{\rho_t\}_{t \geq 1}$ is a sequence of step-sizes satisfying $\sum_{t \geq 1} \rho_t = \infty$ and $\sum_{t \geq 1} \rho_t^2 < \infty$; for example, Hoffman \cite{hoffman2013stochastic} uses $\rho_t = (t + \tau)^{-\kappa}$, with $\tau \geq 0$ and $\kappa \in (0.5, 1]$.
In addition, $\eta^{\text{global}}_b$ is essentially the same as $\eta^{\text{global}}$, except it is computed based on the virtual dataset derived from $\lceil N/S \rceil$ repetitions of the mini-batch $b$.
The overall procedure for stochastic variational inference is outlined in Algorithm~\ref{alg:svi}.

% By using stochastic variational inference, we end up with an algorithm which is, in practice, more efficient than either Gibbs sampling or coordinate-ascent variational inference.
% This is because this algorithm typically requires fewer iterations through the full dataset.
% In particular, it allows us to update our estimates for the global (corpus-wide) latent variables more frequently than once per iteration through the dataset.

\begin{algorithm}[hbt]
\caption{Stochastic Variational Inference}
\label{alg:svi}
\renewcommand{\labelitemi}{{$\bullet$}}
\renewcommand{\labelitemii}{{\tiny$\blacksquare$}}
\begin{itemize}[leftmargin=*]
\item \textbf{Input:}
  \begin{itemize}
  \item $\vec x$: dataset containing $N$ observations
  \end{itemize}
\item \textbf{Output:}
  \begin{itemize}
  \item $\mu$: an optimized parameter corresponding to the global latent variables
  \item $\nu$: a length-$N$ vector of optimized parameters corresponding to the local latent variables
  \end{itemize}
\item \textbf{Procedure:}
\end{itemize}
\begin{algorithmic}
\State Initialize $\mu$ and $\nu$ to ``reasonable'' guesses based on the input data.
\State Let $\{\rho_t\}_{t \geq 1}$ be a sequence of step-sizes satisfying $\sum_{t \geq 1} \rho_t = \infty$ and $\sum_{t \geq 1} \rho_t^2 < \infty$.
\State Set $t \gets 0$
\While{ELBO has not converged}
  \State Update $t \gets t + 1$
  \State Randomly sample a mini-batch $b$ containing $S$ observations
  \For{each observation $x_i$ in $b$}
    \State Update $\nu_i \gets E_q[\eta^{\text{local}}_i]$
  \EndFor
  \State Update $\mu \gets (1-\rho_t)\mu + \rho_t E_q[\eta^{\text{global}}_b]$
  \State Compute ELBO according equation~\eqref{eq:elbo}.
\EndWhile
\State \Return $\mu$, $\nu$
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hierarchical Topic Models Based on Dirichlet Processes}

Next, we discuss a line of research in which random variables known as Dirichlet Processes are used to learn a tree-structured hierarchy of topics from a corpus.
The first such technique is known as the Nested Chinese Restaurant Process, and a subsequent enhancement to this technique is called the Nested Hierarchical Dirichlet Process.

\subsection{The Nested Chinese Restaurant Process Topic Model}

The NCRP topic model was defined in Griffiths et al \cite{griffiths2004hierarchical} and Blei et al \cite{blei2010ncrp} and is capable of learning a hierarchy of topics.
There is some ambiguity in the terminology used in the literature, as the term ``NCRP'' can refer to a probability distribution over paths in a tree, but it has also been used to refer to the particular nested topic model discussed in this section.
To disambiguate these terms, we will typically use the term ``NCRP distribution'' to refer to the former, and ``NCRP topic model'' to refer to the latter.

Both the NCRP distribution and the NCRP topic model are based on a probability distribution known as the Chinese Restaurant Process (CRP), which we will discuss first.

\subsubsection{The Chinese Restaurant Process: A Distribution Over Partitions}

The CRP is a stochastic process which allows us to put a prior distribution on a potentially infinite partition of a dataset.
Here, the term ``stochastic process'' refers to an infinite set of random variables which are indexed by some infinite set.
With nonzero probability, the CRP can assign observations to arbitrarily large numbers of partitions, although for a finite dataset the size of the partition is of course bounded by the number of observations.

The CRP is based on a thought experiment involving a restaurant containing an infinite number of tables, each of which have an infinite capacity.
The tables are indexed starting from 1, and an arbitrary number of customers sequentially arrive at the restaurant.
With probability 1, the first customer is guaranteed to sit at the first table.
All subsequent customers select a table according to the following rules:
\begin{itemize}
\item If $n$ is the total number of previously-seated customers at the restaurant, the $(n+1)\nth$ customer will sit at the next empty table with probability $\frac{\alpha}{n + \alpha}$.
\item If the first $k$ tables are occupied, with the $i\nth$ table containing $m_i$ customers, the $(n+1)\nth$ customer will sit at table $i$ with probability $\frac{m_i}{n+\alpha}$.
\end{itemize}
Here, $\alpha$ is a positive-valued parameter that affects whether there will be relatively few tables containing a large number of customers, or relatively many tables which each contain only a few customers.
In the limit $\alpha \to 0$, there will be a single table with all of the customers.
On the other hand, as $\alpha \to \infty$, the number of occupied tables increases.

Although this thought experiment gives us an intuitive sense for how to generate a random partition, it involves a sequence of random trials in which each trial is dependent on the outcomes of all previous trials.
Thus, for the purpose of deriving an implementation, it is preferable to use a reformulation that involves independent random variables.
One convenient method is to use a \emph{stick-breaking construction}.
We define an infinite sequence of beta-distributed random variables:
\begin{align}
V_k &\sim \text{Beta}(1, \alpha) \qquad \text{for all $k \geq 1$}.
\end{align}
Then, we define an infinite sequence of probabilities:
\begin{align}
\pi_k &= V_k \prod_{j=1}^{k-1} (1-V_j).
\label{eq:pi_crp}
\end{align}
We can view the infinite sequence $\{\pi_k\}_{k \geq 1}$ as defining a probability distribution over all positive integers.
In the limit where the number of observations approaches infinity, the distribution over partitions defined by the CRP and by this stick-breaking construction are equivalent.
Since the random variables $V_k$ are i.i.d., this formulation makes it easier to derive joint and conditional probability distributions.

\subsubsection{The Dirichlet Process: A Distribution Over Grouped Data}

In the context of topic models, the CRP can be used as a distribution over indices into an infinite i.i.d.\ set of topic vectors.
In terms of the restaurant analogy, this corresponds to associating each table with an independent draw from some ``base distribution'' (in topic models, these draws are from a Dirichlet distribution); thus, selecting a table corresponds to selecting a topic vector.
When the CRP is used in this way---specifically, as a distribution over indices into some countably infinite set of i.i.d.\ random variables---we may alternatively view it as a \emph{Dirichlet process}.
The Dirichlet process is a probability distribution over probability distributions.
It is parameterized by a positive scalar parameter $\alpha$ and a base distribution, $G_0$.
As for notation, we can write
\begin{align}
X &\sim \text{DP}(\alpha, G_0)
\end{align}
as shorthand for
\begin{alignat}{2}
\theta_k &\sim G_0 &\qquad& \text{for $k \geq 1$} \\
V_k &\sim \text{Beta}(1, \alpha) &\qquad& \text{for $k \geq 1$} \\
\pi_k &= V_k \prod_{j=1}^{k-1} (1 - V_j) &\qquad& \text{for $k \geq 1$} \\
X &= \sum_{k=1}^\infty \pi_k \delta_{\theta_k} &&
\end{alignat}
Here, the notation $X = \sum_k \pi_k \delta_{\theta_k}$ means that $X$ is a discrete distribution over the values $\{\theta_k : k \geq 1\}$.
In particular, $R \sim X$ implies that $R = \theta_k$ with probability $\sum_{\ell : \theta_k = \theta_\ell} \pi_\ell$, for each $k \geq 1$.
Note that each $\theta_k$ is a member of the domain of the distribution $G_0$, and $X$ is a probability distribution whose domain is a subset of the domain of $G_0$.
Furthermore, $V_k$ and $\pi_k$ are scalars between $0$ and $1$, with the sequence $\{\pi_k\}$ satisfying $\sum_k \pi_k = 1$.
This formulation will be useful in later sections, when we explore the Hierarchical Dirichlet Process (HDP) and the Nested Hierarchical Dirichlet Process (NHDP).

The name ``Dirichlet process'' comes from the fact that, for any finite partition $(A_1, \ldots, A_n)$ of the domain of $G_0$, the vector $(X(A_1), \ldots, X(A_n))$ is disributed according to a Dirichlet distribution with parameters $(\alpha G_0(A_1), \ldots, \alpha G_0(A_n))$.
Here, the notation $X(A_i)$ or $G_0(A_i)$ represents the probability mass which $X$ (or $G_0$, respectively) assigns to the set $A_i$.
The fact that such a distribution exists was proved in Ferguson \cite{ferguson1973bayesian}, although this result was merely an existential proof.
Sethuraman \cite{sethuraman1994constructive} later provided the explicit stick-breaking construction for the Dirichlet process described above.

The preceding formulation can be modified slightly if we express it in terms of a so-called GEM distribution (GEM stands for Griffiths, Engen, and McCloskey).
A GEM distribution is a distribution over distributions over positive integers, defined such that
\begin{align}
\vec \pi &\sim \text{GEM}_2(\gamma_1, \gamma_2)
\end{align}
is equivalent to
\begin{alignat}{2}
V_k &\sim \text{Beta}(\gamma_1, \gamma_2) &\qquad& \text{for $k \geq 1$} \\
\pi_k &= V_k \prod_{j=1}^{k-1} (1 - V_j) &\qquad& \text{for $k \geq 1$}
\end{alignat}
Note in this context that $\vec \pi$ is an infinite sequence; that is, $\vec \pi = \{ \pi_k \}_{k \geq 1}$.
Furthermore, this sequence sums to $1$ and can therefore be viewed as a discrete distribution over the set of positive integers.
The subscript ``2'' indicates the this is a two-parameter variant of the GEM distribution.
The single-parameter variant is equivalent to setting the first parameter to $1$.
That is, $\text{GEM}(\alpha) = \text{GEM}_2(1, \alpha)$.
With these definitions, a Dirichlet process $X \sim \text{DP}(\alpha, G_0)$ is equivalent to
\begin{alignat}{2}
\theta_k &\sim G_0 &\qquad& \text{for $k \geq 1$} \\
\vec \pi &\sim \text{GEM}(\alpha) && \\
X &= \sum_{k=1}^\infty \pi_k \delta_{\theta_k} &&
\end{alignat}

\subsubsection{The Nested Chinese Restaurant Process: A Distribution Over Paths}

Next, we define the Nested Chinese Restaurant Process (NCRP) in terms of the CRP.
Whereas the CRP is a distribution over partitions, the NCRP is a distribution over paths in a tree.
Continuing the restaurant analogy, suppose we have an infinite number of restaurants, each with an infinite number of infinite-capacity tables.
There is one restaurant which is singled out as the ``root'' restaurant, and every customer begins by selecting a restaurant from this table according to the rules of the (non-nested) CRP.
Each table has a card containing the address of another restaurant.
After all customers at the current restaurant have been seated, the customers at each table simultaneously get up and move to the restaurant which is marked on the card at their table.
Then, this process repeats at a new set of restaurants.
No restaurant appears on a card more than once, which means that there is one and only one way to reach each restaurant.

We can imagine each restaurant as a node in an infinitely wide, infinitely deep tree.
Each node can be indexed by a tuple representing the sequence of table selections required to reach that restaurant.
For example, the empty sequence corresponds to the restaurant at the root node, and the sequence $(3, 1)$ corresponds to the first descendant of the third descendant of the root node.
That is, $(3, 1)$ corresponds to the restaurant we would reach if we picked the third table at the root-node restaurant, and then the first table at the next restaurant.

In this formulation, a single draw from the NCRP corresponds to a distribution over infinite paths in this tree.
Equivalently, we can think of a single NCRP draw as a probability distribution over infinite sequences of positive integers.
Drawing an infinite sequence $\{a_i\}_{i \geq 1}$ corresponds to choosing table $a_1$ from the first restaurant, entering the restaurant corresponding to $a_1$, choosing table $a_2$ from that restaurant, entering the restaurant corresponding to $a_2$, choosing table $a_3$ from that restaurant, and so on.

Like the CRP, the NCRP can also be defined in terms of a stick-breaking construction.
To do this, we use an infinite collection of beta-distributed random variables, indexed by the set of finite-length paths in an infinite tree:
\begin{align}
V_r &\sim \text{Beta}(1, \alpha),
\end{align}
where $r$ is any non-empty, finite-length tuple of positive integers.
As a special case, the random variable associated with the root node, $V_{()}$, is always equal to $1$, rather than being beta-distributed.
Then, we recursively define a set of variables as follows:
\begin{align}
\pi_{()} &= V_{()} = 1 \\
\pi_{r[1:\ell]} &= \pi_{r[1:\ell-1]} \cdot \left( V_{r[1:\ell]} \prod_{j=1}^{r[\ell]-1} (1-V_{r[1:\ell-1],j}) \right)
\end{align}
Expanding this recurrence relation yields:
\begin{align}
\pi_r
&= \prod_{\ell = 1}^{\len(r)} \left( V_{r[1:\ell]} \prod_{j=1}^{r[\ell]-1} \left( 1 - V_{r[1:\ell-1],j} \right) \right) \\
&= \left( \prod_{\ell = 1}^{\len(r)} V_{r[1:\ell]} \right) \left( \prod_{\ell = 1}^{\len(r)} \prod_{j=1}^{r[\ell]-1} \left( 1 - V_{r[1:\ell-1],j} \right) \right).
\end{align}
If $r$ is a finite-length tuple, then $\pi_r$ represents the probability of drawing a path which contains the prefix $r$.
Fixing $V_{()} = \pi_{()} = 1$ corresponds to the fact that every path contains the empty tuple as a prefix.
Furthermore, we note the similarity between this formulation and equation \eqref{eq:pi_crp}; in the NCRP, we are essentially repeating the CRP at each node in an infinite tree.

The preceding discussion uses a tree of infinite depth.
However, it is also possible to use only a finite number of layers in the tree (that is, to truncate the tree at a finite depth).
Specifically, if we truncate the tree at depth $L$, then each full-length path in the tree---equivalently, each leaf node in the tree---corresponds to a tuple of positive integers of length $L$.
Note that even if we truncate the tree in terms of its depth, it still contains an infinite number of paths, since the width of the tree is infinite.

\subsubsection{The NCRP Topic Model}

Now we have the necessary ingredients to describe how the NCRP topic model is defined in terms of the NCRP distribution.
In the NCRP topic model, a single draw from the NCRP distribution is used to define a global probability distribution over paths.
Moreover, for each node in the infinite tree, a discrete distribution over the vocabulary is drawn from a Dirichlet distribution.
To each document we assign a single path through the tree, as well as a distribution over ``levels of abstraction'' (i.e., a distribution over depths within the tree).
Then, within each document, each word is drawn using a two step process: first, a depth $z$ is selected from the distribution over depths specific to the current document; next, a vocabulary word is drawn from the $z\nth$ topic along the sampled path corresponding to the current document.

Thus, the generative process for sampling documents according to the NCRP topic model is as follows:
\begin{outline}
\1 For each node in the infinite tree, generate a ``topic,'' which consists of a distribution over the vocabulary, by drawing from a Dirichlet.
To avoid actually computing an infinite number of topics, this step can be performed via lazy evaluation.
That is, we can generate and store a new topic only when a subsequent step requires it.
%
\1 Draw a distribution over paths from the NCRP.
In concrete terms, this means we must draw a beta-distributed random variable for each node in the infinite tree.
Again, this can be done via lazy evaluation.
%
\1 For each document:
  \2 Draw a distribution over depths, using a 2-parameter GEM distribution.  (Alternatively, in the case where the depth has been truncated to some finite value $L$, the distribution over depths can be drawn from a $(L+1)$-component Dirichlet.)
  \2 Draw a path through the global tree, using the global distribution over paths.
  \2 For each word in the document:
    \3 Draw a depth $z$ from the distribution-over-depths associated with this document.
    \3 Draw a vocabulary word from the topic associated with the $z\nth$ element of the path associated with this document.
\end{outline}

Here are the conditional probability distributions for this model:
\begin{align}
\theta_r &\sim \text{Dirichlet}(\alpha^{(\theta)}) \\
V_r &\sim \text{Beta}(1, \alpha^{(V)}) \\
\phi_d &\sim \text{GEM}_2(\alpha^{(\phi)}_1, \alpha^{(\phi)}_2) \\
z_{d,n} &\sim \text{Categorical}(\phi_d) \\
c_d &\sim \sum_{\substack{r:\text{path} \\ \text{of length } L}} \pi_r \delta_r \\
t_{d,n} &\sim \text{Categorical}(\theta[c_d[1:z_{d,n}]])
\shortintertext{where}
\pi_r &= \prod_{\ell=1}^{\len(r)} \left( V_{r[1:\ell]} \prod_{j=1}^{r[\ell]-1} (1 - V_{r[1:\ell-1],j}) \right)
\end{align}
In the case where the depth of the tree has been truncated to $L$, we draw the distributions-over-depths $\phi_d$ from a Dirichlet with $L+1$ components, instead of from a GEM distribution.
We use $L+1$ components because topics can be drawn from anywhere between the root (at depth 0) and the leaves (at depth $L$).
Hence, in the finite-depth case, we would substitute the following distribution for $\phi_d$:
\begin{align}
\phi_d \sim \text{Dirichlet}(\alpha^{(\phi)})
\end{align}
The plate diagram for this graphical model is shown in Figure~\ref{fig:plate-ncrp}.

%%%%
\begin{figure}[htb]
%
\centering
\begin{tikzpicture}
\node[obs] (t) {$t_{d,n}$};
\node[latent, above=of t] (c) {$c_d$};
\node[latent, right=1.5cm of c] (v) {$V_r$};
\node[latent, right=1.5cm of t] (th) {$\theta_r$};
\node[latent, left=of t] (z) {$z_{d,n}$};
\node[latent, above=of z] (phi) {$\phi_d$};
\node[const, left=of phi] (alphaPhi) {$\alpha^{(\phi)}$};
\node[const, right=of v] (alphaV) {$\alpha^{(V)}$};
\node[const, right=of th] (alphaTheta) {$\alpha^{(\theta)}$};

\edge{phi}{z};
\edge{z}{t};
\edge{c}{t};
\edge{v}{c};
\edge{th}{t};
\edge{alphaPhi}{phi};
\edge{alphaV}{v};
\edge{alphaTheta}{th};

\centeredPlate{word-plate}{(t)(z)}{$n \in \{1, \ldots, N_d\}$};
\centeredPlate{doc-plate}{(phi)(c)(z)(t)(word-plate)}{$d \in \{1, \ldots, D\}$};
\centeredPlate{tree-plate}{(th)(v)}{\begin{tabular}{c}$r: $ path to \\ any node\end{tabular}};
\end{tikzpicture}
%
\caption{Plate diagram for the NCRP topic model}
\label{fig:plate-ncrp}
\end{figure}
%%%%

\subsubsection{Gibbs Sampling for the NCRP}

Initially, papers such as \cite{griffiths2004hierarchical} and \cite{blei2010ncrp} used NCRP inference algorithms based on Gibbs sampling.
(In the next section, we will discuss an alternate inference algorithm that was introduced in \cite{wang2009vi_ncrp}.)

%**TODO-actual-algorithm -- discuss what's specific to Gibbs-NCRP

\subsubsection{Variational Inference for the NCRP}

A variational inference algorithm for the NCRP model was proposed in Wang et al \cite{wang2009vi_ncrp}.
The procedure described in section~\ref{sec:cavi} applies to traditional finite-dimensional models;
however, in the context of Bayesian non-parametric models such as the NCRP, the latent variables belong to an infinite-dimensional space.
This means that there are an infinite number of variational parameters that would need to be updated in each iteration.
Hence, in order to obtain a feasible algorithm, we must make additional approximations.
The approach taken in \cite{wang2009vi_ncrp} is to start with a finite version of the tree which has been truncated in terms of both its depth and its width.
The latent variables associated with the nodes that lie outside of this finite truncation are assumed to be equal to the corresponding prior distribution; thus, outside of the finite truncation, there is no dependence on the variational parameters.
In addition to this, the authors also incorporate greedy heuristics for pruning paths, adding paths, and merging paths in the finite tree.
%***TODO CAVI NCRP algorithm

\subsection{The Nested Hierarchical Dirichlet Process Topic Model}

The primary limitation of the NCRP has to do with the fact that each document is associated with only a single path through the topic hierarchy.
This creates problems when the training corpus contains documents with a variety of ``hybrid'' topics.
From the example mentioned earlier, there may be some documents which contain a mixture of the topics \emph{sports} and \emph{medicine}, and the relative proportions of these topics may vary depending on the specific focus of the document.
To accommodate such documents, the NCRP leaves us with limited options:
\begin{enumerate}
\item Treat \emph{sports medicine} as a subtopic of either \emph{sports} or \emph{medicine}.
\item Treat \emph{sports medicine} as a top-level topic, distinct from both \emph{sports} and \emph{medicine}.
\end{enumerate}
Option 1 may lead to unmanageably deep topic hierarchies (or, if we are using the finite-depth variant of NCRP, this option may not even be feasible).
On the other hand, option 2 may lead to unmanageably wide topic hierarchies, particularly if a large number of documents contain hybrids of two or more topics.
In either case, it becomes difficult to represent the topic hierarchy as a relatively small tree.

Rather than face these limited options, we would prefer to augment the model so that it can better handle a variety of hybrid topics.
The solution that was proposed in \cite{paisley2015nhdp} is a topic model called the Nested Hierarchical Dirichlet Process (NHDP).
The NHDP topic model shares many features in common with the NCRP topic model:
\begin{itemize}
\item Once again, we draw a global distribution over paths from an NCRP distribution.
\item As before, there is an infinite set of topic vectors (i.e., distributions over vocabulary words) which are arranged in an infinite tree and are independently drawn from a Dirichlet distribution.
\item Furthermore, each document is associated with a ``local'' distribution over nodes in the infinite tree of topics.
\end{itemize}
The main difference between these two topic models is the way in which the local distributions over nodes are defined.
In the NCRP topic model, each document's local distribution consists of nodes along a single path, but in the NHDP, the local distributions can include multiple branches.
This distinction is illustrated in Figure~\ref{fig:ncrp-vs-nhdp}.

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{../figures/ncrp_vs_nhdp.png}
\caption{Comparison of the NCRP topic model (top row) and the NHDP topic model (bottom row).  %
Within each row are several examples of document-specific distributions over nodes.  %
(Source: Paisley et al \cite{paisley2015nhdp})}
\label{fig:ncrp-vs-nhdp}
\end{figure}

To see how the local, per-document distributions are defined in the NHDP, we must first discuss the (non-nested) Hierarchical Dirichlet Process (HDP).
The HDP was introduced in \cite{teh2005hdp} to model grouped data, in which there are an unbounded (potentially infinite) number of groups, each of which contains multiple observations and is associated with its own vector of parameters.
A simple, non-nested HDP model can be formulated as follows:
\begin{align}
H &\sim \text{DP}(\alpha, G_0) \\
H_d &\sim \text{DP}(\beta, H) \\
\mu_{d,n} &\sim H_d \\
x_{d,n} &\sim F(\mu_{d,n})
\end{align}
In this formulation, there is a base distribution $G_0$ (such as a Gaussian or a Dirichlet), and a ``global'' distribution $H$ is drawn from a Dirichlet process applied to $G_0$.
The distribution $H$ is discrete, which means that it can be viewed as a distribution over countably many independent draws from the base distribution.
Next, per-group distributions $H_d$ (one for each group $d$ in the data) are drawn from another Dirichlet process, which is applied to the global distribution $H$.
For each data point $n$ in group $d$, a parameter vector $\mu_{d,n}$ is drawn from the group-specific distribution $H_d$.
Finally, the observed data points $x_{d,n}$ are drawn according to the distribution $F(\mu_{d,n})$, where $F$ is some parameterized distribution, such as a Gaussian or a categorical distribution.
In the context of topic models, a ``group'' is often defined to be a single document, and the individual observations within each group are the words within the corresponding document.
The NHDP model uses $G_0 = \text{Dirichlet}(\gamma)$ and $F(\cdot) = \text{Categorical}(\cdot)$.

To define a per-document distribution over nodes, the NHDP uses an HDP defined at each node of the infinite topic tree.
However, it also interleaves a document-specific tree of ``path propagation'' random variables to determine, for each word, whether to extend the current path by drawing the next subtopic or to stop at the current node and draw a specific vocabulary word.

The generative process is as follows:
\begin{outline}[enumerate]
\1 For each node $r$ in the infinite tree, draw a topic vector $\theta_r$, representing a distribution over the vocabulary.
Since this involves an infinite number of random draws, this step is performed via lazy evaluation.
\1 For each node $r$ and each $j \geq 1$, draw a beta-distributed stick-breaking proportion $V^*_{r,j}$ via lazy evaluation:
\1 For each document $d$:
  \2 For each node $r$ in the infinite tree, perform the following steps via lazy evaluation:
    \3 For each $j \geq 1$:
      \4 Draw an index $z^d_{r,j} \geq 1$ according to a discrete distribution with probabilities given by $\pi_k = V^*_{r,k} \prod_{i=1}^{k-1} (1-V^*_{r,i})$.
      \4 Draw a beta-distributed stick-breaking proportion $V^d_{r,j}$.
    \3 Draw a beta-distributed path-propagation random variable $U^d_r$.
  \2 For each word-slot $n$ in document $d$:
    \3 Let $r = ()$, the empty path.
    \3 Repeat the following steps until a value for $c^d_n$ is chosen:
      \4 Draw a Bernoulli random variable with probability of success $U^d_r$.
      \4 If the outcome is 1, then set $c^d_n = r$ and break out of this loop; otherwise, continue.
      \4 For all $j \geq 1$ (via lazy evaluation), find all indices $k$ such that $z^d_{r,k} = j$.  Let $S_j$ be the set of such indices, and define $\mu_j = \sum_{k \in S_j} V^d_{r,k} \prod_{i=1}^{k-1} (1-V^d_{r,i})$.
      \4 Draw an index $\tilde z \geq 1$ from a discrete probability distribution with probabilities given by $\mu_j$ as defined above.
      \4 Extend $r$ by appending $\tilde z$ to it; that is, update $r \gets (r, \tilde z)$.
    \3 Draw a vocabulary word $t^d_n$ from a categorical distribution with probabilities given by $\theta_{c^d_n}$.
\end{outline}

In summary, we first draw an infinite, global tree of topics ($\theta_r$) and stick-breaking proportions ($V^*_{r,j}$).
Then, for each document $d$, we draw local sequences of indices ($z^d_{r,j}$) based on the global stick-breaking proportions.
We also draw an independent set of document-specific stick-breaking proportions ($V^d_{r,j}$).
Together, $z^d_{r,j}$ and $V^d_{r,j}$ determine how the topics descended from node $r$ are rearranged and re-weighted in a document-specific fashion.
Furthermore, we draw ``path-propagation'' probabilities $U^d_r$ for each document, and for each node in the infinite tree.
By combining $z^d_{r,j}$, $V^d_{r,j}$, and $U^d_r$, we have a per-document distribution over an infinite tree of nodes.
For each word $n$ in a given document $d$, we draw a node $c^d_n$ from this document-specific distribution over nodes;
this identifies the topic from which the $n\nth$ word of document $d$ will be drawn.
Finally, we draw a vocabulary word $t^d_n$ from ${\text{Categorical}(\theta_{c^d_n})}$.

To visualize how the NHDP model is constructed, Figure~\ref{fig:plate-nhdp} contains a plate diagram of the Bayesian network underlying this model.
Figure~\ref{fig:prob-nhdp} shows the corresponding conditional probability distributions.

%%%%
\begin{figure}[htb]
%
\centering
\begin{tikzpicture}
\node[obs] (t) {$t^d_n$};
\node[latent, left=of t] (c) {$c^d_n$};
\node[latent, left=of c] (v) {$V^d_{r,j}$};
\node[latent, below=of v] (u) {$U^d_r$};
\node[latent, above=of v] (z) {$z^d_{r,j}$};
\node[latent, left=of z] (vstar) {$V^*_{r,j}$};
\node[latent, below=of u] (th) {$\theta_r$};
\node[const, left=of vstar] (alphaVstar) {$\alpha^{(V^*)}$};
\node[const, left=3cm of th] (alphaW) {$\alpha^{(\theta)}$};
\node[const, left=3cm of v] (alphaV) {$\alpha^{(V)}$};
\node[const, left=3cm of u] (alphaU) {$\alpha^{(U)}$};
%
\edge{c}{t};
\edge{z}{c}
\edge{v}{c};
\edge{u}{c};
\edge{vstar}{z};
\edge{th}{t};
\edge{alphaV}{v};
\edge{alphaU}{u};
\edge{alphaVstar}{vstar};
\edge{alphaW}{th};
%
\plate{word-plate}{(t)(c)}{$n \in \{1, \ldots, N_d\}$};
\plate{j-plate}{(z)(vstar)(v)}{$j \geq 1$};
\centeredPlate{tree-plate}{(u)(v)(z)(th)(vstar)(j-plate)}{\begin{tabular}{c}$r: $ path to \\ any node\end{tabular}};
\plate{doc-plate}{(word-plate)(z)(u)(v)}{$d \in \{1, \ldots, D\}$};
\end{tikzpicture}
%
\caption{Plate diagram for the NHDP topic model}
\label{fig:plate-nhdp}
\end{figure}

\begin{figure}[htb]
\centering
\begin{empheq}[box=\fbox]{align*}
\theta_r &\sim \text{Dirichlet}(\alpha^{(\theta)}) \\
V^*_{r,j} &\sim \text{Beta}(1, \alpha^{(V^*)}) \\
V^d_{r,j} &\sim \text{Beta}(1, \alpha^{(V)}) \\
U^d_r &\sim \text{Beta}(\alpha^{(U)}_1, \alpha^{(U)}_2) \\
z^d_{r,j} &\sim \sum_{k \geq 1} \left( V^*_{r,k} \prod_{i=1}^{k-1} (1-V^*_{r,i}) \right) \delta_k \\
c^d_n &\sim \sum_{r: \text{path}} A(r, V^d, z^d) \, B(r, U^d) \, \delta_r \\
A(r, V^d, z^d) &= \prod_{m=0}^{\len(r)-1} \sum_{k \geq 1} \indicator \left[ z^d_{r[1:m],k} = r[m+1] \right] \left( V^d_{r[1:m],k} \prod_{i=1}^{k-1} \left( 1 - V^d_{r[1:m],i} \right) \right) \\
B(r, U^d) &= U^d_r \prod_{m=0}^{\len(r)-1} \left( 1 - U^d_{r[1:m]} \right) \\
t^d_n &\sim \text{Categorical}(\theta_{c^d_n})
\end{empheq}
\caption{Conditional probability distributions for the NHDP topic model}
\label{fig:prob-nhdp}
\end{figure}

\subsubsection{Variational Inference for the NHDP}

The inference algorithm for the NHDP model is based on stochastic variational inference, which was discussed in section~\ref{sec:svi}.
%**TODO aspects of SVI that are specific to NHDP

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Directions for Future Research}

We can identify some directions for future research by considering some of the limitations of the NHDP topic model.

\textbf{Scalable algorithms:}
The currently available set of inference algorithms force us to make a significant tradeoff between precision and scalability.
On the one hand, we have Gibbs sampling, which can theoretically compute the true posterior distribution with arbitrary precision, but which requires requires many passes over the training data and therefore fails to scale well to massive datasets.
On the other hand, we have stochastic variational inference, which uses a more efficient mini-batch approach and has deterministic convergence; however, it requires us to make major approximating assumptions, such as the mean-field approximation.
To improve the inference algorithm, we could perhaps look for a variant of Gibbs sampling that requires fewer passes over the training data.
Alternatively, we might seek a modified version of stochastic variational inference which makes less stringent assumptions on the form of the variational distributions.
Yet another approach might be to extend the non-probabilistic, geometric ``anchor words'' approach of Arora et al \cite{arora2013practical} in order to handle hierarchies of topics.

\textbf{Interpreting models:}
The models discussed above generally represent topics as discrete probability distributions over the entire vocabulary.
However, a typical vocabulary may contain tens of thousands of words, which makes the vector of topic probabilities unwieldy and difficult to interpret.
A common approach is to visualize a topic by displaying its top $K$ most likely vocabulary words.
However, within a large topic hierarchy, a small value of $K$ might not provide enough information to disambiguate closely related topics, whereas a large value may lead to information overload.
By comparison, human-curated topic hierarchies such as library cataloguing systems often use a very small number of words to describe each node in a large hierarchy.
Hence, it might be useful to develop techniques to post-process the results of these models in order to derive a concise yet meaningful description of each topic.

\textbf{Incorporating human feedback:}
In cases where the model's quality is deemed subpar, it may be useful to be able to incorporate human feedback.
This would require not only an investigation of the types of mistakes which these topic models are prone to make in practice, but also an investigation into how best to design a user interface that is capable of gathering useful feedback.
It may also be useful to apply semi-supervised or multi-label classification techniques in scenarios where partial or even contradictory labels are available.

\textbf{Moving beyond the bag-of-words model:}
All of the models discussed above ignore the order in which words appear in a document.
The sequence of words in a document can influence its meaning, and therefore its classification within a topic hierarchy.
By making use of word-order information, we could potentially improve the quality of the results.

\textbf{Frameworks for Bayesian non-parametric inference:}
Deriving inference algorithms for Bayesian non-parametric models, such as the ones discussed above, is a laborious process which is difficult to automate.
For instance, even though the NCRP and NHDP topic models share many features, their inference algorithms must be derived in a model-specific way.
This is made especially difficult by the fact that, in Bayesian non-parametric models, there can be an infinite number of latent variables.
This requires us to augment traditional inference algorithms with model-specific heuristics.
An approach that combines black-box inference techniques (such as those described in \cite{ranganath2014blackbox} and \cite{ranganath2016operator}) with model-agnostic heuristics for handling infinite-dimensional latent variables might make it feasible to efficiently explore and compare many variants of these models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\nocite{*}
%\bibliographystyle{plainnat}
\bibliographystyle{plain}
\bibliography{../bibliography}

\end{document}
