\documentclass{article}

\usepackage[final]{nips_2016_modified}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{geometry}
\geometry{margin=1.5in}

\newcommand{\nth}{^{\text{th}}}

\title{Hierarchical Topic Models}

\author{
  Andrew Leverentz \\
  \texttt{aleveren@eng.ucsd.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
In this article, we survey techniques for automatically inferring hierarchical structures from unlabeled collections of text documents.
We begin with a brief overview of probabilistic topic modeling, as implemented in models such as Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA).
After discussing some limitations of these models, we discuss two lines of research which have extended the LDA model and attempted to address some of its weaknesses.
One approach, based on Dirichlet Processes, treats topics as belonging to a potentially infinite tree; in this context, Bayesian non-parametric techniques are used to automatically select the structure of the tree.
The second approach, called Hierarchical Pachinko Allocation Modeling, uses a directed acyclic graph to recursively define topics as distributions not only over words but also over other topics.
We compare and contrast these approaches and discuss several inference algorithms that have been published in the literature.
%We also discuss methods for visualizing, interpreting, and evaluating hierarchies of topics.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

In recent decades, information-sharing networks such as the World Wide Web and other digital archives have led to the creation of vast collections of electronic textual data.
As these collections grow, the problem of efficiently discovering relationships between documents, or between  user queries and documents, has become more prominent.
The field of \emph{topic modeling} has produced several automated approaches to solving this problem.
One underlying theme of topic modeling is the notion that words with similar meanings tend to co-occur with similar sets of words.
However, due to ambiguities in natural language and variances between different authors, these co-occurrence patterns are not rigid and deterministic.
Hence, many techniques within the field of topic modeling use a probabilistic framework.
This probabilistic framework typically treats topics as discrete probability distributions over vocabulary words, and documents may contain mixtures of multiple topics.

In subsequent sections, we briefly review the origins of Latent Dirichlet Allocation (LDA) and the probabilistic framework for topic modeling.
Then, we trace the development of two lines of research which extend the LDA model and are capable of producing not just a flat collection of topics but a nested hierarchy of topics.
We conclude with a discussion of potential avenues for future research.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation}
Two of the main applications of topic models are (1) discovering relations between documents and (2) indexing large collections of documents.

\paragraph{Discovering relations between documents:}
One approach is to formulate this as a clustering task with soft cluster assignments, where each document can be assigned to a mixture of multiple topics or clusters.
For example, a document about a basketball player recovering from an injury might be represented as a mixture between topics corresponding to both \emph{sports} and \emph{medicine}.

\paragraph{Indexing large collections of documents:}
In the context of information retrieval, the task is to take a user query (expressed perhaps as a collection of keywords or as a natural-language sentence) and efficiently find a set of documents which are deemed most relevant to that query.

These tasks are complicated by ambiguities in natural language.
In particular, \emph{synonymy} (in which multiple words have nearly identical meanings) and \emph{polysemy} (in which a single word may have multiple meanings, depending on the context) make it difficult to draw conclusions about relationships between documents based on the specific set of words used in each document.
For example, two closely related documents might express nearly the same concept using completely different words, whereas two unrelated documents might coincidentally both use a particular word in different senses.
Because of this, it is necessary to move beyond superficial representations of documents (such as strings of characters or tokens, or histograms of vocabulary words) and instead represent documents using some notion of ``latent semantics,'' or underlying meaning.

Topic models accomplish this by treating topics as probability distributions over the vocabulary and allowing documents to draw from a mixture of multiple topics.
For example, a topic relating to sports would assign relatively high probability to words such as ``player,'' ``team,'' and ``game.''
Similarly, a topic about medicine would assign relatively high probability to ``doctor,'' ``illness,'' and ``pharmacy.''
Then, a document about a basketball player recovering from an injury might consist of $70\%$ \emph{sports} and $30\%$ \emph{medicine}, whereas a document about physical therapy for athletes might consist of $70\%$ \emph{medicine} and $30\%$ \emph{sports}.

Hierarchical topic models take this idea further by acknowledging that whether or not two documents share the same topic may depend on the level of abstraction that the user is interested in.
By explicitly treating the set of available topics as a tree, hierarchical topic models can therefore represent documents as mixtures of nested topics at varying levels of abstraction.
For indexing tasks, hierarchical topic models allow the possibility of expanding or narrowing the set of relevant documents, depending on the level of abstraction desired by the user.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Probabilistic Topic Models}

Before we discuss models which can generate hierarchies of topics, we will first discuss two probabilistic models which generate ``flat'' (that is, non-nested) collections of topics.
These models are Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA).

\subsection{A Non-Probabilistic Precursor: Latent Semantic Analysis}

The first of these, PLSA, was inspired by an earlier, non-probabilistic model known as Latent Semantic Analysis (LSA).
In LSA, a corpus of documents is summarized as a matrix; each row of this matrix represents a document, each column represents a term in the vocabulary, and each entry represents number of times a given term appears in a given document.
If the corpus contains a sufficiently diverse set of documents, then most documents will only contain a small subset of the full vocabulary, and the resulting matrix will be sparse.
After constructing this matrix, it is then transformed according to per-term and per-document statistics.
(Several variants of LSA exist which use different transformations in this step, although a common choice is known as ``tf-idf,'' or ``term frequency--inverse document frequency.'')
Then, the singular value decomposition (SVD) of the transformed matrix is computed.
This represents the matrix as a product of three matrices: a matrix of orthogonal row vectors where each row corresponds to a document, a diagonal matrix containing the so-called singular values, and a matrix of orthogonal column vectors where each column corresponds to a term in the corpus.
If we compute the SVD using a low-rank approximation, this corresponds to truncating the row vectors and the column vectors in the SVD; at the same time, we also discard all but the largest singular values.
The truncated row vectors from the SVD are known as ``latent semantic vectors.''
These vectors are viewed as representations of the documents in a low-dimensional ``latent semantic space.''
Similarities between documents can be computed, for example, using a normalized dot product (also known as the cosine similarity) in the latent semantic space.
This model, as with many other topic models, uses the \emph{bag-of-words} simplification, in which the specific sequence of words in a document is ignored, and all that matters is the frequency of terms within each document.
Topic models which eliminate the bag-of-words simplification are beyond the scope of this review, but they constitute an active area of research.

\subsection{Probabilistic Latent Semantic Analysis}

In abstract terms, PLSA takes a similar conceptual approach, but the latent semantic representations of topics are combined in each document using a generative probabilistic model rather than simply a linear algebraic one.
PLSA uses what Hofmann (**citation1999) calls an ``aspect model.''
In the aspect model, each document $d$ is associated with a discrete probability distribution $p(z \mid d)$ over $z \in \mathcal Z$, where $\mathcal Z$ is a set of latent factors which are referred to as ``topics.''
Each topic $z \in \mathcal Z$ is in turn associated with a distribution $p(w \mid z)$ over words $w$ in the vocabulary $\mathcal W$.
Thus, the joint probability of documents in a corpus and the words they contain is given by
\begin{align*}
p(w, d)
&= \sum_{z \in \mathcal Z} p(w, z, d) \\
&= p(d) \sum_{z \in \mathcal Z} p(w \mid z) \, p(z \mid d).
\end{align*}

Within this framework, it becomes possible to infer the latent class mixture of each document using a likelihood-optimization algorithm, such as EM (expectation maximization).

It is also worth noting that the ``A'' in LSA and PLSA is sometimes replaced with ``I'' for ``Indexing'' when the primary goal is to efficiently retrieve documents related to a given user query.
Thus the terms ``LSI'' and ``PLSI'' are often used in the Information Retrieval research community.

\subsection{Latent Dirichlet Allocation}

Latent Dirichlet Allocation (LDA) extends PLSA by using the Dirichlet distribution as a Bayesian prior on the space of discrete probability distributions.
The generative model used in LDA is given by:
\begin{align*}
\theta_k &\sim \text{Dirichlet}(\alpha) \qquad \text{for each topic $k$} \\
z_{d,n} &\sim \text{Categorical}(\beta) \qquad \text{for the $n\nth$ word in document $d$} \\
w_{d,n} &\sim \text{Categorical}(\theta_{z_{d,n}}) \qquad \text{for the $n\nth$ word in document $d$}
\end{align*}
In other words, the per-topic probability distributions over the vocabulary are i.i.d.\ draws from a Dirichlet distribution.
Then, within each document, we select a topic indicator $z_{d,n}$ for each word according to the probabilities in by the vector $\beta$, and the vocabulary word $w_{d,n} \in \mathcal W$ at the $n\nth$ position is selected according to the probabilities in the vector $\theta_{z_{d,n}}$.
The main advantage of this formulation is that the parameters associated with each topic $\theta_k$ (corresponding to the probability distributions $p(w \mid z)$ in PLSA) do not need to be specified in advance;
instead, they can be inferred from the training data.

Several important limitations of the LDA model are:
\begin{enumerate}
\item The number of topics must be specified in advance.
Setting this parameter using standard model-selection techniques such as $k$-fold cross-validation can be time consuming.
We will see that this can be addressed via Bayesian nonparametric techniques, in which one or more latent variables are drawn from an infinite-dimensional space.
\item A ``flat'' collection of topics has no built-in notion of levels of abstraction, and so the topics discovered by LDA may be difficult to interpret.
This is because such models may mix abstract and concrete terms within a single topic.
Nested topics allow us to represent containment relationships between relatively abstract topics, such as ``sports in general'' or ``basketball'' and relatively concrete topics, such as ``Michael Jordan'' or ``the 1996 Chicago Bulls.''
The intention is to create more easily interpretable topics, each of which can be associated with a sharply distinguished level of abstraction.
\item The bag-of-words approach fails to account for the ordering of words, even though this may significantly impact the meaning of a document; for example, in the bag-of-words approach ``man bites dog'' is treated as exactly equivalent to ``dog bites man.''
Solutions to this limitation are outside the scope of this report, although as mentioned above this is an active area of research.
\end{enumerate}

\subsection{Inference Algorithms for LDA}

A number of inference algorithms for LDA have been proposed, including EM (expectation--maximization), Gibbs sampling, and variational inference.
We will see that some of the same algorithms can be applied to hierarchical topic models, although our choice of algorithms is constrained somewhat by the increased complexity of these models.

Expectation--maximization (EM) is an example of a general class of optimization algorithms known as majorization--maximization (MM).
It can be used to find maximum likelihood estimates or maximum a posteriori estimates for latent variables.
However, in a fully Bayesian setting, we are often looking for more than a mere point estimate; rather, we seek to approximate the posterior distribution of the latent variables given the observed data.

Gibbs sampling is a particular kind of stochastic sampling technique which can be used in the context of Bayesian modeling to estimate the posterior distribution.
It belongs to the class of Monte Carlo Markov Chain (MCMC) techniques.
This is a ``Monte Carlo'' technique because it involves estimating a quantity by repeatedly drawing samples of a set of random variables.
It is a ``Markov Chain'' technique because it involves a sequence of stochastic updates in which the transition probabilities at each step depend only upon the previous state.
To implement Gibbs sampling, it is necessary to compute the conditional probability distribution of each latent variable given all other latent variables, plus the full set of observed variables.
A single iteration of the Markov chain update in Gibbs sampling involves taking each latent variable one at a time (possibly in a random order), temporarily discarding its current value, computing the conditional probability distribution of the current latent variable, and sampling an updated value from that conditional distribution.
The update rules needed for implementing this technique are often relatively easy to derive (compared to alternatives such as variational inference).
However, the main disadvantage is that it can be difficult to determine when the Markov chain has converged to the true posterior distribution.

Variational inference is an optimization technique that can be used to find an approximation to the posterior distribution.
Typically, it involves searching within a restricted family of functions (such as those which decompose conveniently into a product of univariate factors) to find the function which most closely approximates the true posterior.
The ``closeness'' between probability distributions can be measured by the Kullbach-Leibler divergence.
One common form of variational inference is based on coordinate ascent.
This approach updates the parameters associated with each variational distribution, one at a time, until it has converged to a local optimum.
Coordinate ascent variational inference is particularly convenient for models which use conditional distributions from the exponential family, and which make use of conjugate priors wherever possible.
In such cases, we can derive a simple closed-form update rule for each of the latent variables.
We will see that Bayesian nonparametric models, which involve latent variables drawn from infinite dimensional spaces, require special handling.
For example, incorporating a greedy algorithm into the update rule is one way to avoid computing infinite sums.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hierarchical Topic Models Based on Dirichlet Processes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hierarchical Topic Models Based on Pachinko Allocation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Potential Directions for Future Research}
TODO
In \cite{paisley2015nested}, there is some stuff.
Similarly, in \cite{blei2010nested}, there is some stuff.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\nocite{*}
%\bibliographystyle{plainnat}
\bibliographystyle{plain}
\bibliography{../bibliography}

\end{document}
